{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffic_data import METRLADatasetLoader # descarga de datos\n",
    "import numpy as np # manipulacion de datos\n",
    "import matplotlib.pyplot as plt # graficar\n",
    "import torch # crear modelos\n",
    "from torch.utils.data import TensorDataset, DataLoader # manipular dataset\n",
    "import torch.nn as nn # para usar las capas predefinidas en torch\n",
    "import torch.optim as optim # para poder utilizar adam como el optimizador\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data, Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los datos\n",
    "loader = METRLADatasetLoader() \n",
    "adj, weig, x, y = loader.get_dataset(num_timesteps_in=12, \n",
    "                                     num_timesteps_out=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omitiremos la variable del tiempo\n",
    "x = [i[:, 0, :] for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = torch.tensor(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34260 34260\n",
      "(207, 12)\n",
      "(207, 1)\n"
     ]
    }
   ],
   "source": [
    "print(len(x), len(y)) # instancias para entrenamiento\n",
    "print(x[0].shape) # cada instancia tiene 207 nodos en 12 momentos\n",
    "print(y[0].shape) # el ground truth es el grafo en el siguiente momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34260, 12, 207, 1])\n",
      "torch.Size([34260, 207, 1])\n"
     ]
    }
   ],
   "source": [
    "# convertimos la lista a un tensor aumentando una dimension mas\n",
    "# primero convertimos a array porque es mas eficiente \n",
    "X = torch.tensor(np.array(x)).permute(0, 2, 1).unsqueeze(-1)\n",
    "Y = torch.tensor(np.array(y))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# particionamos en train y test y cargamos los datos en objetos\n",
    "# DataLoader para mejorar la eficiencia\n",
    "\n",
    "train_p = 0.8 # porcentaje de training\n",
    "batch_size = 128\n",
    "\n",
    "train_size = int(train_p * len(X))\n",
    "test_size = len(X) - train_size\n",
    "\n",
    "X_train, X_test = torch.split(X, [train_size, test_size])\n",
    "Y_train, Y_test = torch.split(Y, [train_size, test_size])\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "test_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                          shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                         shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27408, 12, 207, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class GAT_LSTM_Model(nn.Module):\n",
    "    def __init__(self, node_feature_size, hidden_dim, lstm_layers, output_size, num_nodes):\n",
    "        super(GAT_LSTM_Model, self).__init__()\n",
    "        self.gat_conv = GATConv(node_feature_size, hidden_dim, heads=1)\n",
    "        self.lstm = nn.LSTM(hidden_dim * num_nodes, hidden_dim, lstm_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size * num_nodes)\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "    def forward(self, graph_sequence, edge_index):\n",
    "        batch_size, seq_len, num_nodes, node_feature_size = graph_sequence.size()\n",
    "\n",
    "        # Preparar las secuencias de grafos para GATConv\n",
    "        graph_sequence = graph_sequence.view(batch_size * seq_len, num_nodes, node_feature_size)\n",
    "        x = graph_sequence.view(-1, node_feature_size)  # (batch_size * seq_len * num_nodes) x node_feature_size\n",
    "\n",
    "        # Ajustar edge_index para batch processing\n",
    "        edge_index_batch = edge_index.repeat(1, batch_size * seq_len)\n",
    "        offset = torch.arange(0, batch_size * seq_len * num_nodes, step=num_nodes, dtype=torch.long).repeat_interleave(edge_index.size(1))\n",
    "        offset = offset.to(device)\n",
    "        edge_index_batch = edge_index_batch + offset\n",
    "\n",
    "        # Procesar todos los grafos en la secuencia de una vez con GATConv\n",
    "        gat_output = self.gat_conv(x, edge_index_batch)\n",
    "        gat_output = gat_output.view(batch_size, seq_len, num_nodes, -1) # -1 = hidden_dim para las features de cada nodo\n",
    "\n",
    "        # Pasar las salidas de GATConv a LSTM\n",
    "        gat_output = gat_output.view(batch_size, seq_len, -1)  # batch_size x seq_len x (num_nodes * hidden_dim)\n",
    "        lstm_out, _ = self.lstm(gat_output)\n",
    "\n",
    "        # Predecir el siguiente estado del grafo\n",
    "        lstm_out = lstm_out[:, -1, :]  # batch_size x hidden_dim\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out.view(batch_size, num_nodes, -1)  # batch_size x num_nodes x output_size\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature_size = 1 \n",
    "hidden_dim = 1\n",
    "lstm_layers = 1\n",
    "output_size = 1\n",
    "num_nodes = X_train.size(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = GAT_LSTM_Model(node_feature_size, hidden_dim, lstm_layers, output_size, num_nodes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "adj = adj.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20000], Loss: 0.8802\n",
      "Epoch [2/20000], Loss: 0.9915\n",
      "Epoch [3/20000], Loss: 0.7059\n",
      "Epoch [4/20000], Loss: 0.7322\n",
      "Epoch [5/20000], Loss: 0.8052\n",
      "Epoch [6/20000], Loss: 0.8268\n",
      "Epoch [7/20000], Loss: 0.8265\n",
      "Epoch [8/20000], Loss: 0.9937\n",
      "Epoch [9/20000], Loss: 0.7687\n",
      "Epoch [10/20000], Loss: 0.9067\n",
      "Epoch [11/20000], Loss: 0.8382\n",
      "Epoch [12/20000], Loss: 0.8051\n",
      "Epoch [13/20000], Loss: 0.9094\n",
      "Epoch [14/20000], Loss: 0.6508\n",
      "Epoch [15/20000], Loss: 0.7248\n",
      "Epoch [16/20000], Loss: 0.7834\n",
      "Epoch [17/20000], Loss: 0.8349\n",
      "Epoch [18/20000], Loss: 0.8844\n",
      "Epoch [19/20000], Loss: 1.0876\n",
      "Epoch [20/20000], Loss: 0.9207\n",
      "Epoch [21/20000], Loss: 0.8050\n",
      "Epoch [22/20000], Loss: 0.7496\n",
      "Epoch [23/20000], Loss: 0.6837\n",
      "Epoch [24/20000], Loss: 0.9451\n",
      "Epoch [25/20000], Loss: 1.1426\n",
      "Epoch [26/20000], Loss: 0.6430\n",
      "Epoch [27/20000], Loss: 0.8243\n",
      "Epoch [28/20000], Loss: 0.8028\n",
      "Epoch [29/20000], Loss: 0.8164\n",
      "Epoch [30/20000], Loss: 0.6000\n",
      "Epoch [31/20000], Loss: 0.9766\n",
      "Epoch [32/20000], Loss: 0.9490\n",
      "Epoch [33/20000], Loss: 0.7647\n",
      "Epoch [34/20000], Loss: 0.9150\n",
      "Epoch [35/20000], Loss: 0.6990\n",
      "Epoch [36/20000], Loss: 0.5678\n",
      "Epoch [37/20000], Loss: 0.6732\n",
      "Epoch [38/20000], Loss: 0.8376\n",
      "Epoch [39/20000], Loss: 0.8549\n",
      "Epoch [40/20000], Loss: 0.7389\n",
      "Epoch [41/20000], Loss: 0.7734\n",
      "Epoch [42/20000], Loss: 0.9117\n",
      "Epoch [43/20000], Loss: 0.9022\n",
      "Epoch [44/20000], Loss: 1.0202\n",
      "Epoch [45/20000], Loss: 0.7171\n",
      "Epoch [46/20000], Loss: 0.9963\n",
      "Epoch [47/20000], Loss: 0.9852\n",
      "Epoch [48/20000], Loss: 0.7426\n",
      "Epoch [49/20000], Loss: 0.7584\n",
      "Epoch [50/20000], Loss: 0.9461\n",
      "Epoch [51/20000], Loss: 0.6987\n",
      "Epoch [52/20000], Loss: 0.5249\n",
      "Epoch [53/20000], Loss: 0.6467\n",
      "Epoch [54/20000], Loss: 0.8210\n",
      "Epoch [55/20000], Loss: 0.8696\n",
      "Epoch [56/20000], Loss: 0.9331\n",
      "Epoch [57/20000], Loss: 0.6753\n",
      "Epoch [58/20000], Loss: 0.9765\n",
      "Epoch [59/20000], Loss: 0.7228\n",
      "Epoch [60/20000], Loss: 0.9567\n",
      "Epoch [61/20000], Loss: 0.7460\n",
      "Epoch [62/20000], Loss: 0.9027\n",
      "Epoch [63/20000], Loss: 0.8009\n",
      "Epoch [64/20000], Loss: 0.6592\n",
      "Epoch [65/20000], Loss: 0.7481\n",
      "Epoch [66/20000], Loss: 0.7812\n",
      "Epoch [67/20000], Loss: 0.8335\n",
      "Epoch [68/20000], Loss: 0.6479\n",
      "Epoch [69/20000], Loss: 0.5654\n",
      "Epoch [70/20000], Loss: 0.7109\n",
      "Epoch [71/20000], Loss: 0.6520\n",
      "Epoch [72/20000], Loss: 0.7093\n",
      "Epoch [73/20000], Loss: 0.6990\n",
      "Epoch [74/20000], Loss: 0.5283\n",
      "Epoch [75/20000], Loss: 0.4916\n",
      "Epoch [76/20000], Loss: 0.7300\n",
      "Epoch [77/20000], Loss: 0.6492\n",
      "Epoch [78/20000], Loss: 0.7504\n",
      "Epoch [79/20000], Loss: 0.6369\n",
      "Epoch [80/20000], Loss: 0.8838\n",
      "Epoch [81/20000], Loss: 0.6087\n",
      "Epoch [82/20000], Loss: 0.7621\n",
      "Epoch [83/20000], Loss: 0.6874\n",
      "Epoch [84/20000], Loss: 0.5271\n",
      "Epoch [85/20000], Loss: 0.7367\n",
      "Epoch [86/20000], Loss: 0.6198\n",
      "Epoch [87/20000], Loss: 0.6320\n",
      "Epoch [88/20000], Loss: 0.5144\n",
      "Epoch [89/20000], Loss: 0.6333\n",
      "Epoch [90/20000], Loss: 0.5647\n",
      "Epoch [91/20000], Loss: 0.6594\n",
      "Epoch [92/20000], Loss: 0.6192\n",
      "Epoch [93/20000], Loss: 0.6937\n",
      "Epoch [94/20000], Loss: 0.6907\n",
      "Epoch [95/20000], Loss: 0.6007\n",
      "Epoch [96/20000], Loss: 0.6767\n",
      "Epoch [97/20000], Loss: 0.5463\n",
      "Epoch [98/20000], Loss: 0.6153\n",
      "Epoch [99/20000], Loss: 0.7492\n",
      "Epoch [100/20000], Loss: 0.5602\n",
      "Epoch [101/20000], Loss: 0.5376\n",
      "Epoch [102/20000], Loss: 0.6130\n",
      "Epoch [103/20000], Loss: 0.6454\n",
      "Epoch [104/20000], Loss: 0.6195\n",
      "Epoch [105/20000], Loss: 0.6463\n",
      "Epoch [106/20000], Loss: 0.5085\n",
      "Epoch [107/20000], Loss: 0.4355\n",
      "Epoch [108/20000], Loss: 0.5404\n",
      "Epoch [109/20000], Loss: 0.5890\n",
      "Epoch [110/20000], Loss: 0.5896\n",
      "Epoch [111/20000], Loss: 0.4429\n",
      "Epoch [112/20000], Loss: 0.6469\n",
      "Epoch [113/20000], Loss: 0.6643\n",
      "Epoch [114/20000], Loss: 0.6408\n",
      "Epoch [115/20000], Loss: 0.6370\n",
      "Epoch [116/20000], Loss: 0.4601\n",
      "Epoch [117/20000], Loss: 0.5610\n",
      "Epoch [118/20000], Loss: 0.4650\n",
      "Epoch [119/20000], Loss: 0.5666\n",
      "Epoch [120/20000], Loss: 0.5983\n",
      "Epoch [121/20000], Loss: 0.5188\n",
      "Epoch [122/20000], Loss: 0.6422\n",
      "Epoch [123/20000], Loss: 0.5235\n",
      "Epoch [124/20000], Loss: 0.4864\n",
      "Epoch [125/20000], Loss: 0.5239\n",
      "Epoch [126/20000], Loss: 0.4354\n",
      "Epoch [127/20000], Loss: 0.4661\n",
      "Epoch [128/20000], Loss: 0.5062\n",
      "Epoch [129/20000], Loss: 0.4825\n",
      "Epoch [130/20000], Loss: 0.4376\n",
      "Epoch [131/20000], Loss: 0.4722\n",
      "Epoch [132/20000], Loss: 0.4495\n",
      "Epoch [133/20000], Loss: 0.5020\n",
      "Epoch [134/20000], Loss: 0.4417\n",
      "Epoch [135/20000], Loss: 0.4779\n",
      "Epoch [136/20000], Loss: 0.4859\n",
      "Epoch [137/20000], Loss: 0.5605\n",
      "Epoch [138/20000], Loss: 0.5277\n",
      "Epoch [139/20000], Loss: 0.5390\n",
      "Epoch [140/20000], Loss: 0.4857\n",
      "Epoch [141/20000], Loss: 0.4445\n",
      "Epoch [142/20000], Loss: 0.5497\n",
      "Epoch [143/20000], Loss: 0.5333\n",
      "Epoch [144/20000], Loss: 0.5133\n",
      "Epoch [145/20000], Loss: 0.4977\n",
      "Epoch [146/20000], Loss: 0.4406\n",
      "Epoch [147/20000], Loss: 0.4680\n",
      "Epoch [148/20000], Loss: 0.4007\n",
      "Epoch [149/20000], Loss: 0.6288\n",
      "Epoch [150/20000], Loss: 0.5260\n",
      "Epoch [151/20000], Loss: 0.5428\n",
      "Epoch [152/20000], Loss: 0.4962\n",
      "Epoch [153/20000], Loss: 0.4568\n",
      "Epoch [154/20000], Loss: 0.5330\n",
      "Epoch [155/20000], Loss: 0.4995\n",
      "Epoch [156/20000], Loss: 0.4915\n",
      "Epoch [157/20000], Loss: 0.4948\n",
      "Epoch [158/20000], Loss: 0.4737\n",
      "Epoch [159/20000], Loss: 0.4812\n",
      "Epoch [160/20000], Loss: 0.5542\n",
      "Epoch [161/20000], Loss: 0.4925\n",
      "Epoch [162/20000], Loss: 0.4076\n",
      "Epoch [163/20000], Loss: 0.4529\n",
      "Epoch [164/20000], Loss: 0.4615\n",
      "Epoch [165/20000], Loss: 0.4293\n",
      "Epoch [166/20000], Loss: 0.5328\n",
      "Epoch [167/20000], Loss: 0.5019\n",
      "Epoch [168/20000], Loss: 0.4132\n",
      "Epoch [169/20000], Loss: 0.4409\n",
      "Epoch [170/20000], Loss: 0.4439\n",
      "Epoch [171/20000], Loss: 0.5332\n",
      "Epoch [172/20000], Loss: 0.5347\n",
      "Epoch [173/20000], Loss: 0.4443\n",
      "Epoch [174/20000], Loss: 0.3812\n",
      "Epoch [175/20000], Loss: 0.4667\n",
      "Epoch [176/20000], Loss: 0.5306\n",
      "Epoch [177/20000], Loss: 0.4826\n",
      "Epoch [178/20000], Loss: 0.3758\n",
      "Epoch [179/20000], Loss: 0.4875\n",
      "Epoch [180/20000], Loss: 0.4337\n",
      "Epoch [181/20000], Loss: 0.4080\n",
      "Epoch [182/20000], Loss: 0.5585\n",
      "Epoch [183/20000], Loss: 0.4626\n",
      "Epoch [184/20000], Loss: 0.4906\n",
      "Epoch [185/20000], Loss: 0.3878\n",
      "Epoch [186/20000], Loss: 0.4787\n",
      "Epoch [187/20000], Loss: 0.4899\n",
      "Epoch [188/20000], Loss: 0.4728\n",
      "Epoch [189/20000], Loss: 0.3998\n",
      "Epoch [190/20000], Loss: 0.4025\n",
      "Epoch [191/20000], Loss: 0.4579\n",
      "Epoch [192/20000], Loss: 0.4313\n",
      "Epoch [193/20000], Loss: 0.4151\n",
      "Epoch [194/20000], Loss: 0.4896\n",
      "Epoch [195/20000], Loss: 0.3785\n",
      "Epoch [196/20000], Loss: 0.3842\n",
      "Epoch [197/20000], Loss: 0.4604\n",
      "Epoch [198/20000], Loss: 0.4663\n",
      "Epoch [199/20000], Loss: 0.4277\n",
      "Epoch [200/20000], Loss: 0.4772\n",
      "Epoch [201/20000], Loss: 0.4934\n",
      "Epoch [202/20000], Loss: 0.4204\n",
      "Epoch [203/20000], Loss: 0.3910\n",
      "Epoch [204/20000], Loss: 0.3701\n",
      "Epoch [205/20000], Loss: 0.4183\n",
      "Epoch [206/20000], Loss: 0.5272\n",
      "Epoch [207/20000], Loss: 0.3994\n",
      "Epoch [208/20000], Loss: 0.4250\n",
      "Epoch [209/20000], Loss: 0.4113\n",
      "Epoch [210/20000], Loss: 0.4203\n",
      "Epoch [211/20000], Loss: 0.4959\n",
      "Epoch [212/20000], Loss: 0.4237\n",
      "Epoch [213/20000], Loss: 0.4235\n",
      "Epoch [214/20000], Loss: 0.3582\n",
      "Epoch [215/20000], Loss: 0.3908\n",
      "Epoch [216/20000], Loss: 0.4728\n",
      "Epoch [217/20000], Loss: 0.4810\n",
      "Epoch [218/20000], Loss: 0.4155\n",
      "Epoch [219/20000], Loss: 0.4438\n",
      "Epoch [220/20000], Loss: 0.4369\n",
      "Epoch [221/20000], Loss: 0.4756\n",
      "Epoch [222/20000], Loss: 0.4592\n",
      "Epoch [223/20000], Loss: 0.4100\n",
      "Epoch [224/20000], Loss: 0.3982\n",
      "Epoch [225/20000], Loss: 0.3842\n",
      "Epoch [226/20000], Loss: 0.4600\n",
      "Epoch [227/20000], Loss: 0.3946\n",
      "Epoch [228/20000], Loss: 0.4638\n",
      "Epoch [229/20000], Loss: 0.4592\n",
      "Epoch [230/20000], Loss: 0.3860\n",
      "Epoch [231/20000], Loss: 0.4262\n",
      "Epoch [232/20000], Loss: 0.4737\n",
      "Epoch [233/20000], Loss: 0.3806\n",
      "Epoch [234/20000], Loss: 0.5018\n",
      "Epoch [235/20000], Loss: 0.4335\n",
      "Epoch [236/20000], Loss: 0.4332\n",
      "Epoch [237/20000], Loss: 0.3881\n",
      "Epoch [238/20000], Loss: 0.4378\n",
      "Epoch [239/20000], Loss: 0.4237\n",
      "Epoch [240/20000], Loss: 0.5539\n",
      "Epoch [241/20000], Loss: 0.4406\n",
      "Epoch [242/20000], Loss: 0.3824\n",
      "Epoch [243/20000], Loss: 0.3795\n",
      "Epoch [244/20000], Loss: 0.3560\n",
      "Epoch [245/20000], Loss: 0.3772\n",
      "Epoch [246/20000], Loss: 0.4231\n",
      "Epoch [247/20000], Loss: 0.4148\n",
      "Epoch [248/20000], Loss: 0.4030\n",
      "Epoch [249/20000], Loss: 0.4741\n",
      "Epoch [250/20000], Loss: 0.4067\n",
      "Epoch [251/20000], Loss: 0.4766\n",
      "Epoch [252/20000], Loss: 0.5565\n",
      "Epoch [253/20000], Loss: 0.3646\n",
      "Epoch [254/20000], Loss: 0.3678\n",
      "Epoch [255/20000], Loss: 0.3548\n",
      "Epoch [256/20000], Loss: 0.3694\n",
      "Epoch [257/20000], Loss: 0.3786\n",
      "Epoch [258/20000], Loss: 0.3931\n",
      "Epoch [259/20000], Loss: 0.4386\n",
      "Epoch [260/20000], Loss: 0.3694\n",
      "Epoch [261/20000], Loss: 0.5244\n",
      "Epoch [262/20000], Loss: 0.4201\n",
      "Epoch [263/20000], Loss: 0.5114\n",
      "Epoch [264/20000], Loss: 0.4065\n",
      "Epoch [265/20000], Loss: 0.3918\n",
      "Epoch [266/20000], Loss: 0.3927\n",
      "Epoch [267/20000], Loss: 0.3669\n",
      "Epoch [268/20000], Loss: 0.4026\n",
      "Epoch [269/20000], Loss: 0.4229\n",
      "Epoch [270/20000], Loss: 0.3563\n",
      "Epoch [271/20000], Loss: 0.4038\n",
      "Epoch [272/20000], Loss: 0.3989\n",
      "Epoch [273/20000], Loss: 0.3617\n",
      "Epoch [274/20000], Loss: 0.3613\n",
      "Epoch [275/20000], Loss: 0.4006\n",
      "Epoch [276/20000], Loss: 0.3914\n",
      "Epoch [277/20000], Loss: 0.3733\n",
      "Epoch [278/20000], Loss: 0.3648\n",
      "Epoch [279/20000], Loss: 0.4369\n",
      "Epoch [280/20000], Loss: 0.4462\n",
      "Epoch [281/20000], Loss: 0.3925\n",
      "Epoch [282/20000], Loss: 0.4411\n",
      "Epoch [283/20000], Loss: 0.4851\n",
      "Epoch [284/20000], Loss: 0.5867\n",
      "Epoch [285/20000], Loss: 0.4922\n",
      "Epoch [286/20000], Loss: 0.6331\n",
      "Epoch [287/20000], Loss: 0.4256\n",
      "Epoch [288/20000], Loss: 0.3873\n",
      "Epoch [289/20000], Loss: 0.3813\n",
      "Epoch [290/20000], Loss: 0.4182\n",
      "Epoch [291/20000], Loss: 0.4951\n",
      "Epoch [292/20000], Loss: 0.3749\n",
      "Epoch [293/20000], Loss: 0.4383\n",
      "Epoch [294/20000], Loss: 0.4030\n",
      "Epoch [295/20000], Loss: 0.3980\n",
      "Epoch [296/20000], Loss: 0.4526\n",
      "Epoch [297/20000], Loss: 0.4349\n",
      "Epoch [298/20000], Loss: 0.3944\n",
      "Epoch [299/20000], Loss: 0.3610\n",
      "Epoch [300/20000], Loss: 0.3479\n",
      "Epoch [301/20000], Loss: 0.3504\n",
      "Epoch [302/20000], Loss: 0.3726\n",
      "Epoch [303/20000], Loss: 0.4527\n",
      "Epoch [304/20000], Loss: 0.4403\n",
      "Epoch [305/20000], Loss: 0.4079\n",
      "Epoch [306/20000], Loss: 0.3741\n",
      "Epoch [307/20000], Loss: 0.4829\n",
      "Epoch [308/20000], Loss: 0.4100\n",
      "Epoch [309/20000], Loss: 0.4982\n",
      "Epoch [310/20000], Loss: 0.3878\n",
      "Epoch [311/20000], Loss: 0.3776\n",
      "Epoch [312/20000], Loss: 0.3512\n",
      "Epoch [313/20000], Loss: 0.4140\n",
      "Epoch [314/20000], Loss: 0.4346\n",
      "Epoch [315/20000], Loss: 0.3340\n",
      "Epoch [316/20000], Loss: 0.3671\n",
      "Epoch [317/20000], Loss: 0.3754\n",
      "Epoch [318/20000], Loss: 0.4284\n",
      "Epoch [319/20000], Loss: 0.3799\n",
      "Epoch [320/20000], Loss: 0.3965\n",
      "Epoch [321/20000], Loss: 0.4396\n",
      "Epoch [322/20000], Loss: 0.3851\n",
      "Epoch [323/20000], Loss: 0.3513\n",
      "Epoch [324/20000], Loss: 0.3664\n",
      "Epoch [325/20000], Loss: 0.4416\n",
      "Epoch [326/20000], Loss: 0.3924\n",
      "Epoch [327/20000], Loss: 0.4080\n",
      "Epoch [328/20000], Loss: 0.3369\n",
      "Epoch [329/20000], Loss: 0.4358\n",
      "Epoch [330/20000], Loss: 0.3732\n",
      "Epoch [331/20000], Loss: 0.4337\n",
      "Epoch [332/20000], Loss: 0.3898\n",
      "Epoch [333/20000], Loss: 0.4295\n",
      "Epoch [334/20000], Loss: 0.3699\n",
      "Epoch [335/20000], Loss: 0.3324\n",
      "Epoch [336/20000], Loss: 0.3515\n",
      "Epoch [337/20000], Loss: 0.3659\n",
      "Epoch [338/20000], Loss: 0.4284\n",
      "Epoch [339/20000], Loss: 0.3789\n",
      "Epoch [340/20000], Loss: 0.3645\n",
      "Epoch [341/20000], Loss: 0.3839\n",
      "Epoch [342/20000], Loss: 0.4735\n",
      "Epoch [343/20000], Loss: 0.3607\n",
      "Epoch [344/20000], Loss: 0.3693\n",
      "Epoch [345/20000], Loss: 0.4326\n",
      "Epoch [346/20000], Loss: 0.3640\n",
      "Epoch [347/20000], Loss: 0.3612\n",
      "Epoch [348/20000], Loss: 0.4390\n",
      "Epoch [349/20000], Loss: 0.3988\n",
      "Epoch [350/20000], Loss: 0.3522\n",
      "Epoch [351/20000], Loss: 0.3478\n",
      "Epoch [352/20000], Loss: 0.3751\n",
      "Epoch [353/20000], Loss: 0.3613\n",
      "Epoch [354/20000], Loss: 0.3861\n",
      "Epoch [355/20000], Loss: 0.3307\n",
      "Epoch [356/20000], Loss: 0.4379\n",
      "Epoch [357/20000], Loss: 0.4451\n",
      "Epoch [358/20000], Loss: 0.3443\n",
      "Epoch [359/20000], Loss: 0.3519\n",
      "Epoch [360/20000], Loss: 0.3580\n",
      "Epoch [361/20000], Loss: 0.3838\n",
      "Epoch [362/20000], Loss: 0.4813\n",
      "Epoch [363/20000], Loss: 0.3726\n",
      "Epoch [364/20000], Loss: 0.3936\n",
      "Epoch [365/20000], Loss: 0.3659\n",
      "Epoch [366/20000], Loss: 0.3778\n",
      "Epoch [367/20000], Loss: 0.3943\n",
      "Epoch [368/20000], Loss: 0.3517\n",
      "Epoch [369/20000], Loss: 0.3953\n",
      "Epoch [370/20000], Loss: 0.4049\n",
      "Epoch [371/20000], Loss: 0.4764\n",
      "Epoch [372/20000], Loss: 0.4232\n",
      "Epoch [373/20000], Loss: 0.3295\n",
      "Epoch [374/20000], Loss: 0.4575\n",
      "Epoch [375/20000], Loss: 0.4830\n",
      "Epoch [376/20000], Loss: 0.4154\n",
      "Epoch [377/20000], Loss: 0.3585\n",
      "Epoch [378/20000], Loss: 0.4306\n",
      "Epoch [379/20000], Loss: 0.3343\n",
      "Epoch [380/20000], Loss: 0.3884\n",
      "Epoch [381/20000], Loss: 0.3632\n",
      "Epoch [382/20000], Loss: 0.3738\n",
      "Epoch [383/20000], Loss: 0.3525\n",
      "Epoch [384/20000], Loss: 0.3568\n",
      "Epoch [385/20000], Loss: 0.5359\n",
      "Epoch [386/20000], Loss: 0.4391\n",
      "Epoch [387/20000], Loss: 0.3829\n",
      "Epoch [388/20000], Loss: 0.3487\n",
      "Epoch [389/20000], Loss: 0.3699\n",
      "Epoch [390/20000], Loss: 0.4611\n",
      "Epoch [391/20000], Loss: 0.3934\n",
      "Epoch [392/20000], Loss: 0.4295\n",
      "Epoch [393/20000], Loss: 0.4667\n",
      "Epoch [394/20000], Loss: 0.4089\n",
      "Epoch [395/20000], Loss: 0.3753\n",
      "Epoch [396/20000], Loss: 0.4509\n",
      "Epoch [397/20000], Loss: 0.4259\n",
      "Epoch [398/20000], Loss: 0.4557\n",
      "Epoch [399/20000], Loss: 0.3382\n",
      "Epoch [400/20000], Loss: 0.3718\n",
      "Epoch [401/20000], Loss: 0.4972\n",
      "Epoch [402/20000], Loss: 0.3648\n",
      "Epoch [403/20000], Loss: 0.4866\n",
      "Epoch [404/20000], Loss: 0.4418\n",
      "Epoch [405/20000], Loss: 0.3947\n",
      "Epoch [406/20000], Loss: 0.3778\n",
      "Epoch [407/20000], Loss: 0.4068\n",
      "Epoch [408/20000], Loss: 0.4047\n",
      "Epoch [409/20000], Loss: 0.3819\n",
      "Epoch [410/20000], Loss: 0.3571\n",
      "Epoch [411/20000], Loss: 0.4593\n",
      "Epoch [412/20000], Loss: 0.4517\n",
      "Epoch [413/20000], Loss: 0.4185\n",
      "Epoch [414/20000], Loss: 0.4376\n",
      "Epoch [415/20000], Loss: 0.3740\n",
      "Epoch [416/20000], Loss: 0.3552\n",
      "Epoch [417/20000], Loss: 0.3682\n",
      "Epoch [418/20000], Loss: 0.3322\n",
      "Epoch [419/20000], Loss: 0.4391\n",
      "Epoch [420/20000], Loss: 0.3770\n",
      "Epoch [421/20000], Loss: 0.3490\n",
      "Epoch [422/20000], Loss: 0.4157\n",
      "Epoch [423/20000], Loss: 0.3983\n",
      "Epoch [424/20000], Loss: 0.4017\n",
      "Epoch [425/20000], Loss: 0.3633\n",
      "Epoch [426/20000], Loss: 0.4993\n",
      "Epoch [427/20000], Loss: 0.4389\n",
      "Epoch [428/20000], Loss: 0.4124\n",
      "Epoch [429/20000], Loss: 0.4117\n",
      "Epoch [430/20000], Loss: 0.4814\n",
      "Epoch [431/20000], Loss: 0.3642\n",
      "Epoch [432/20000], Loss: 0.4112\n",
      "Epoch [433/20000], Loss: 0.4203\n",
      "Epoch [434/20000], Loss: 0.4828\n",
      "Epoch [435/20000], Loss: 0.3304\n",
      "Epoch [436/20000], Loss: 0.4106\n",
      "Epoch [437/20000], Loss: 0.3368\n",
      "Epoch [438/20000], Loss: 0.4179\n",
      "Epoch [439/20000], Loss: 0.3475\n",
      "Epoch [440/20000], Loss: 0.3375\n",
      "Epoch [441/20000], Loss: 0.3447\n",
      "Epoch [442/20000], Loss: 0.3606\n",
      "Epoch [443/20000], Loss: 0.3898\n",
      "Epoch [444/20000], Loss: 0.4595\n",
      "Epoch [445/20000], Loss: 0.3693\n",
      "Epoch [446/20000], Loss: 0.4023\n",
      "Epoch [447/20000], Loss: 0.3422\n",
      "Epoch [448/20000], Loss: 0.3598\n",
      "Epoch [449/20000], Loss: 0.3780\n",
      "Epoch [450/20000], Loss: 0.3645\n",
      "Epoch [451/20000], Loss: 0.4764\n",
      "Epoch [452/20000], Loss: 0.3671\n",
      "Epoch [453/20000], Loss: 0.5621\n",
      "Epoch [454/20000], Loss: 0.3774\n",
      "Epoch [455/20000], Loss: 0.6594\n",
      "Epoch [456/20000], Loss: 0.4427\n",
      "Epoch [457/20000], Loss: 0.4091\n",
      "Epoch [458/20000], Loss: 0.4140\n",
      "Epoch [459/20000], Loss: 0.3498\n",
      "Epoch [460/20000], Loss: 0.3627\n",
      "Epoch [461/20000], Loss: 0.4883\n",
      "Epoch [462/20000], Loss: 0.4446\n",
      "Epoch [463/20000], Loss: 0.3216\n",
      "Epoch [464/20000], Loss: 0.5006\n",
      "Epoch [465/20000], Loss: 0.4087\n",
      "Epoch [466/20000], Loss: 0.3724\n",
      "Epoch [467/20000], Loss: 0.3393\n",
      "Epoch [468/20000], Loss: 0.4725\n",
      "Epoch [469/20000], Loss: 0.4126\n",
      "Epoch [470/20000], Loss: 0.3238\n",
      "Epoch [471/20000], Loss: 0.4159\n",
      "Epoch [472/20000], Loss: 0.4163\n",
      "Epoch [473/20000], Loss: 0.3675\n",
      "Epoch [474/20000], Loss: 0.4339\n",
      "Epoch [475/20000], Loss: 0.3889\n",
      "Epoch [476/20000], Loss: 0.3257\n",
      "Epoch [477/20000], Loss: 0.4063\n",
      "Epoch [478/20000], Loss: 0.3948\n",
      "Epoch [479/20000], Loss: 0.3925\n",
      "Epoch [480/20000], Loss: 0.5228\n",
      "Epoch [481/20000], Loss: 0.3631\n",
      "Epoch [482/20000], Loss: 0.3637\n",
      "Epoch [483/20000], Loss: 0.3368\n",
      "Epoch [484/20000], Loss: 0.4561\n",
      "Epoch [485/20000], Loss: 0.3892\n",
      "Epoch [486/20000], Loss: 0.3556\n",
      "Epoch [487/20000], Loss: 0.4438\n",
      "Epoch [488/20000], Loss: 0.4191\n",
      "Epoch [489/20000], Loss: 0.3639\n",
      "Epoch [490/20000], Loss: 0.3486\n",
      "Epoch [491/20000], Loss: 0.3576\n",
      "Epoch [492/20000], Loss: 0.4315\n",
      "Epoch [493/20000], Loss: 0.3362\n",
      "Epoch [494/20000], Loss: 0.3352\n",
      "Epoch [495/20000], Loss: 0.4411\n",
      "Epoch [496/20000], Loss: 0.3965\n",
      "Epoch [497/20000], Loss: 0.3852\n",
      "Epoch [498/20000], Loss: 0.3867\n",
      "Epoch [499/20000], Loss: 0.4524\n",
      "Epoch [500/20000], Loss: 0.4516\n",
      "Epoch [501/20000], Loss: 0.3270\n",
      "Epoch [502/20000], Loss: 0.3875\n",
      "Epoch [503/20000], Loss: 0.3698\n",
      "Epoch [504/20000], Loss: 0.3794\n",
      "Epoch [505/20000], Loss: 0.3562\n",
      "Epoch [506/20000], Loss: 0.4239\n",
      "Epoch [507/20000], Loss: 0.4154\n",
      "Epoch [508/20000], Loss: 0.3739\n",
      "Epoch [509/20000], Loss: 0.4383\n",
      "Epoch [510/20000], Loss: 0.4459\n",
      "Epoch [511/20000], Loss: 0.3474\n",
      "Epoch [512/20000], Loss: 0.4294\n",
      "Epoch [513/20000], Loss: 0.4312\n",
      "Epoch [514/20000], Loss: 0.3587\n",
      "Epoch [515/20000], Loss: 0.3697\n",
      "Epoch [516/20000], Loss: 0.4323\n",
      "Epoch [517/20000], Loss: 0.4201\n",
      "Epoch [518/20000], Loss: 0.4419\n",
      "Epoch [519/20000], Loss: 0.4057\n",
      "Epoch [520/20000], Loss: 0.3518\n",
      "Epoch [521/20000], Loss: 0.4890\n",
      "Epoch [522/20000], Loss: 0.3378\n",
      "Epoch [523/20000], Loss: 0.3883\n",
      "Epoch [524/20000], Loss: 0.4301\n",
      "Epoch [525/20000], Loss: 0.3721\n",
      "Epoch [526/20000], Loss: 0.5315\n",
      "Epoch [527/20000], Loss: 0.3374\n",
      "Epoch [528/20000], Loss: 0.3535\n",
      "Epoch [529/20000], Loss: 0.3233\n",
      "Epoch [530/20000], Loss: 0.4168\n",
      "Epoch [531/20000], Loss: 0.4244\n",
      "Epoch [532/20000], Loss: 0.3588\n",
      "Epoch [533/20000], Loss: 0.3584\n",
      "Epoch [534/20000], Loss: 0.3454\n",
      "Epoch [535/20000], Loss: 0.4531\n",
      "Epoch [536/20000], Loss: 0.4088\n",
      "Epoch [537/20000], Loss: 0.4439\n",
      "Epoch [538/20000], Loss: 0.6068\n",
      "Epoch [539/20000], Loss: 0.5399\n",
      "Epoch [540/20000], Loss: 0.3375\n",
      "Epoch [541/20000], Loss: 0.3486\n",
      "Epoch [542/20000], Loss: 0.4899\n",
      "Epoch [543/20000], Loss: 0.4548\n",
      "Epoch [544/20000], Loss: 0.4115\n",
      "Epoch [545/20000], Loss: 0.3831\n",
      "Epoch [546/20000], Loss: 0.3648\n",
      "Epoch [547/20000], Loss: 0.3641\n",
      "Epoch [548/20000], Loss: 0.4535\n",
      "Epoch [549/20000], Loss: 0.4248\n",
      "Epoch [550/20000], Loss: 0.4704\n",
      "Epoch [551/20000], Loss: 0.3036\n",
      "Epoch [552/20000], Loss: 0.4152\n",
      "Epoch [553/20000], Loss: 0.3292\n",
      "Epoch [554/20000], Loss: 0.4300\n",
      "Epoch [555/20000], Loss: 0.3605\n",
      "Epoch [556/20000], Loss: 0.4044\n",
      "Epoch [557/20000], Loss: 0.3738\n",
      "Epoch [558/20000], Loss: 0.3615\n",
      "Epoch [559/20000], Loss: 0.3527\n",
      "Epoch [560/20000], Loss: 0.3353\n",
      "Epoch [561/20000], Loss: 0.3678\n",
      "Epoch [562/20000], Loss: 0.3927\n",
      "Epoch [563/20000], Loss: 0.3194\n",
      "Epoch [564/20000], Loss: 0.3719\n",
      "Epoch [565/20000], Loss: 0.3839\n",
      "Epoch [566/20000], Loss: 0.3440\n",
      "Epoch [567/20000], Loss: 0.4052\n",
      "Epoch [568/20000], Loss: 0.4420\n",
      "Epoch [569/20000], Loss: 0.4274\n",
      "Epoch [570/20000], Loss: 0.4501\n",
      "Epoch [571/20000], Loss: 0.3834\n",
      "Epoch [572/20000], Loss: 0.4916\n",
      "Epoch [573/20000], Loss: 0.3676\n",
      "Epoch [574/20000], Loss: 0.3677\n",
      "Epoch [575/20000], Loss: 0.3361\n",
      "Epoch [576/20000], Loss: 0.4008\n",
      "Epoch [577/20000], Loss: 0.3215\n",
      "Epoch [578/20000], Loss: 0.3940\n",
      "Epoch [579/20000], Loss: 0.4010\n",
      "Epoch [580/20000], Loss: 0.3364\n",
      "Epoch [581/20000], Loss: 0.3512\n",
      "Epoch [582/20000], Loss: 0.3914\n",
      "Epoch [583/20000], Loss: 0.3455\n",
      "Epoch [584/20000], Loss: 0.4505\n",
      "Epoch [585/20000], Loss: 0.3569\n",
      "Epoch [586/20000], Loss: 0.4481\n",
      "Epoch [587/20000], Loss: 0.3392\n",
      "Epoch [588/20000], Loss: 0.4345\n",
      "Epoch [589/20000], Loss: 0.4593\n",
      "Epoch [590/20000], Loss: 0.4025\n",
      "Epoch [591/20000], Loss: 0.4737\n",
      "Epoch [592/20000], Loss: 0.3767\n",
      "Epoch [593/20000], Loss: 0.3706\n",
      "Epoch [594/20000], Loss: 0.4368\n",
      "Epoch [595/20000], Loss: 0.4641\n",
      "Epoch [596/20000], Loss: 0.4176\n",
      "Epoch [597/20000], Loss: 0.4120\n",
      "Epoch [598/20000], Loss: 0.3269\n",
      "Epoch [599/20000], Loss: 0.3772\n",
      "Epoch [600/20000], Loss: 0.3365\n",
      "Epoch [601/20000], Loss: 0.3812\n",
      "Epoch [602/20000], Loss: 0.4091\n",
      "Epoch [603/20000], Loss: 0.4977\n",
      "Epoch [604/20000], Loss: 0.5620\n",
      "Epoch [605/20000], Loss: 0.4095\n",
      "Epoch [606/20000], Loss: 0.4288\n",
      "Epoch [607/20000], Loss: 0.4165\n",
      "Epoch [608/20000], Loss: 0.4367\n",
      "Epoch [609/20000], Loss: 0.3864\n",
      "Epoch [610/20000], Loss: 0.5128\n",
      "Epoch [611/20000], Loss: 0.3831\n",
      "Epoch [612/20000], Loss: 0.3338\n",
      "Epoch [613/20000], Loss: 0.3808\n",
      "Epoch [614/20000], Loss: 0.3766\n",
      "Epoch [615/20000], Loss: 0.4410\n",
      "Epoch [616/20000], Loss: 0.3588\n",
      "Epoch [617/20000], Loss: 0.4311\n",
      "Epoch [618/20000], Loss: 0.4254\n",
      "Epoch [619/20000], Loss: 0.4827\n",
      "Epoch [620/20000], Loss: 0.3535\n",
      "Epoch [621/20000], Loss: 0.3315\n",
      "Epoch [622/20000], Loss: 0.3344\n",
      "Epoch [623/20000], Loss: 0.4878\n",
      "Epoch [624/20000], Loss: 0.3833\n",
      "Epoch [625/20000], Loss: 0.3091\n",
      "Epoch [626/20000], Loss: 0.4885\n",
      "Epoch [627/20000], Loss: 0.3689\n",
      "Epoch [628/20000], Loss: 0.3723\n",
      "Epoch [629/20000], Loss: 0.3363\n",
      "Epoch [630/20000], Loss: 0.3219\n",
      "Epoch [631/20000], Loss: 0.4067\n",
      "Epoch [632/20000], Loss: 0.4119\n",
      "Epoch [633/20000], Loss: 0.4388\n",
      "Epoch [634/20000], Loss: 0.3671\n",
      "Epoch [635/20000], Loss: 0.3334\n",
      "Epoch [636/20000], Loss: 0.3501\n",
      "Epoch [637/20000], Loss: 0.5156\n",
      "Epoch [638/20000], Loss: 0.4277\n",
      "Epoch [639/20000], Loss: 0.3607\n",
      "Epoch [640/20000], Loss: 0.3983\n",
      "Epoch [641/20000], Loss: 0.3729\n",
      "Epoch [642/20000], Loss: 0.4344\n",
      "Epoch [643/20000], Loss: 0.3670\n",
      "Epoch [644/20000], Loss: 0.3382\n",
      "Epoch [645/20000], Loss: 0.4277\n",
      "Epoch [646/20000], Loss: 0.4399\n",
      "Epoch [647/20000], Loss: 0.5048\n",
      "Epoch [648/20000], Loss: 0.4265\n",
      "Epoch [649/20000], Loss: 0.4007\n",
      "Epoch [650/20000], Loss: 0.4167\n",
      "Epoch [651/20000], Loss: 0.4420\n",
      "Epoch [652/20000], Loss: 0.3545\n",
      "Epoch [653/20000], Loss: 0.3977\n",
      "Epoch [654/20000], Loss: 0.3814\n",
      "Epoch [655/20000], Loss: 0.4517\n",
      "Epoch [656/20000], Loss: 0.4362\n",
      "Epoch [657/20000], Loss: 0.3471\n",
      "Epoch [658/20000], Loss: 0.3510\n",
      "Epoch [659/20000], Loss: 0.3725\n",
      "Epoch [660/20000], Loss: 0.4131\n",
      "Epoch [661/20000], Loss: 0.3751\n",
      "Epoch [662/20000], Loss: 0.4069\n",
      "Epoch [663/20000], Loss: 0.5165\n",
      "Epoch [664/20000], Loss: 0.3574\n",
      "Epoch [665/20000], Loss: 0.3421\n",
      "Epoch [666/20000], Loss: 0.6005\n",
      "Epoch [667/20000], Loss: 0.4339\n",
      "Epoch [668/20000], Loss: 0.4224\n",
      "Epoch [669/20000], Loss: 0.3925\n",
      "Epoch [670/20000], Loss: 0.4554\n",
      "Epoch [671/20000], Loss: 0.4281\n",
      "Epoch [672/20000], Loss: 0.3491\n",
      "Epoch [673/20000], Loss: 0.3589\n",
      "Epoch [674/20000], Loss: 0.3134\n",
      "Epoch [675/20000], Loss: 0.4530\n",
      "Epoch [676/20000], Loss: 0.3909\n",
      "Epoch [677/20000], Loss: 0.5028\n",
      "Epoch [678/20000], Loss: 0.4091\n",
      "Epoch [679/20000], Loss: 0.3639\n",
      "Epoch [680/20000], Loss: 0.4017\n",
      "Epoch [681/20000], Loss: 0.3502\n",
      "Epoch [682/20000], Loss: 0.4735\n",
      "Epoch [683/20000], Loss: 0.4196\n",
      "Epoch [684/20000], Loss: 0.3421\n",
      "Epoch [685/20000], Loss: 0.3596\n",
      "Epoch [686/20000], Loss: 0.4663\n",
      "Epoch [687/20000], Loss: 0.3797\n",
      "Epoch [688/20000], Loss: 0.3760\n",
      "Epoch [689/20000], Loss: 0.3615\n",
      "Epoch [690/20000], Loss: 0.4075\n",
      "Epoch [691/20000], Loss: 0.3782\n",
      "Epoch [692/20000], Loss: 0.3644\n",
      "Epoch [693/20000], Loss: 0.3265\n",
      "Epoch [694/20000], Loss: 0.3788\n",
      "Epoch [695/20000], Loss: 0.4805\n",
      "Epoch [696/20000], Loss: 0.5394\n",
      "Epoch [697/20000], Loss: 0.3614\n",
      "Epoch [698/20000], Loss: 0.3520\n",
      "Epoch [699/20000], Loss: 0.3832\n",
      "Epoch [700/20000], Loss: 0.4035\n",
      "Epoch [701/20000], Loss: 0.4268\n",
      "Epoch [702/20000], Loss: 0.3245\n",
      "Epoch [703/20000], Loss: 0.4414\n",
      "Epoch [704/20000], Loss: 0.3722\n",
      "Epoch [705/20000], Loss: 0.4094\n",
      "Epoch [706/20000], Loss: 0.5043\n",
      "Epoch [707/20000], Loss: 0.4411\n",
      "Epoch [708/20000], Loss: 0.4124\n",
      "Epoch [709/20000], Loss: 0.4132\n",
      "Epoch [710/20000], Loss: 0.3629\n",
      "Epoch [711/20000], Loss: 0.4441\n",
      "Epoch [712/20000], Loss: 0.3662\n",
      "Epoch [713/20000], Loss: 0.3799\n",
      "Epoch [714/20000], Loss: 0.4170\n",
      "Epoch [715/20000], Loss: 0.3549\n",
      "Epoch [716/20000], Loss: 0.3526\n",
      "Epoch [717/20000], Loss: 0.3524\n",
      "Epoch [718/20000], Loss: 0.4215\n",
      "Epoch [719/20000], Loss: 0.3819\n",
      "Epoch [720/20000], Loss: 0.3921\n",
      "Epoch [721/20000], Loss: 0.3960\n",
      "Epoch [722/20000], Loss: 0.4576\n",
      "Epoch [723/20000], Loss: 0.4645\n",
      "Epoch [724/20000], Loss: 0.3270\n",
      "Epoch [725/20000], Loss: 0.4863\n",
      "Epoch [726/20000], Loss: 0.3752\n",
      "Epoch [727/20000], Loss: 0.4289\n",
      "Epoch [728/20000], Loss: 0.3814\n",
      "Epoch [729/20000], Loss: 0.3242\n",
      "Epoch [730/20000], Loss: 0.5332\n",
      "Epoch [731/20000], Loss: 0.3306\n",
      "Epoch [732/20000], Loss: 0.3843\n",
      "Epoch [733/20000], Loss: 0.5070\n",
      "Epoch [734/20000], Loss: 0.4096\n",
      "Epoch [735/20000], Loss: 0.3484\n",
      "Epoch [736/20000], Loss: 0.3776\n",
      "Epoch [737/20000], Loss: 0.4094\n",
      "Epoch [738/20000], Loss: 0.4317\n",
      "Epoch [739/20000], Loss: 0.4204\n",
      "Epoch [740/20000], Loss: 0.5095\n",
      "Epoch [741/20000], Loss: 0.5277\n",
      "Epoch [742/20000], Loss: 0.3463\n",
      "Epoch [743/20000], Loss: 0.5068\n",
      "Epoch [744/20000], Loss: 0.3677\n",
      "Epoch [745/20000], Loss: 0.3555\n",
      "Epoch [746/20000], Loss: 0.5795\n",
      "Epoch [747/20000], Loss: 0.3426\n",
      "Epoch [748/20000], Loss: 0.3946\n",
      "Epoch [749/20000], Loss: 0.4019\n",
      "Epoch [750/20000], Loss: 0.4117\n",
      "Epoch [751/20000], Loss: 0.3930\n",
      "Epoch [752/20000], Loss: 0.3334\n",
      "Epoch [753/20000], Loss: 0.3792\n",
      "Epoch [754/20000], Loss: 0.3451\n",
      "Epoch [755/20000], Loss: 0.3085\n",
      "Epoch [756/20000], Loss: 0.3822\n",
      "Epoch [757/20000], Loss: 0.3194\n",
      "Epoch [758/20000], Loss: 0.3855\n",
      "Epoch [759/20000], Loss: 0.4002\n",
      "Epoch [760/20000], Loss: 0.3900\n",
      "Epoch [761/20000], Loss: 0.3344\n",
      "Epoch [762/20000], Loss: 0.3794\n",
      "Epoch [763/20000], Loss: 0.3597\n",
      "Epoch [764/20000], Loss: 0.3280\n",
      "Epoch [765/20000], Loss: 0.3648\n",
      "Epoch [766/20000], Loss: 0.3421\n",
      "Epoch [767/20000], Loss: 0.4103\n",
      "Epoch [768/20000], Loss: 0.4058\n",
      "Epoch [769/20000], Loss: 0.3942\n",
      "Epoch [770/20000], Loss: 0.3451\n",
      "Epoch [771/20000], Loss: 0.4588\n",
      "Epoch [772/20000], Loss: 0.3993\n",
      "Epoch [773/20000], Loss: 0.4436\n",
      "Epoch [774/20000], Loss: 0.4145\n",
      "Epoch [775/20000], Loss: 0.4373\n",
      "Epoch [776/20000], Loss: 0.3364\n",
      "Epoch [777/20000], Loss: 0.4088\n",
      "Epoch [778/20000], Loss: 0.3410\n",
      "Epoch [779/20000], Loss: 0.4615\n",
      "Epoch [780/20000], Loss: 0.4077\n",
      "Epoch [781/20000], Loss: 0.3520\n",
      "Epoch [782/20000], Loss: 0.3691\n",
      "Epoch [783/20000], Loss: 0.4346\n",
      "Epoch [784/20000], Loss: 0.3789\n",
      "Epoch [785/20000], Loss: 0.3798\n",
      "Epoch [786/20000], Loss: 0.4233\n",
      "Epoch [787/20000], Loss: 0.3572\n",
      "Epoch [788/20000], Loss: 0.4151\n",
      "Epoch [789/20000], Loss: 0.3891\n",
      "Epoch [790/20000], Loss: 0.4166\n",
      "Epoch [791/20000], Loss: 0.3256\n",
      "Epoch [792/20000], Loss: 0.4149\n",
      "Epoch [793/20000], Loss: 0.3432\n",
      "Epoch [794/20000], Loss: 0.4267\n",
      "Epoch [795/20000], Loss: 0.3212\n",
      "Epoch [796/20000], Loss: 0.4384\n",
      "Epoch [797/20000], Loss: 0.3448\n",
      "Epoch [798/20000], Loss: 0.3750\n",
      "Epoch [799/20000], Loss: 0.3973\n",
      "Epoch [800/20000], Loss: 0.3969\n",
      "Epoch [801/20000], Loss: 0.4253\n",
      "Epoch [802/20000], Loss: 0.4001\n",
      "Epoch [803/20000], Loss: 0.3690\n",
      "Epoch [804/20000], Loss: 0.4145\n",
      "Epoch [805/20000], Loss: 0.3990\n",
      "Epoch [806/20000], Loss: 0.4305\n",
      "Epoch [807/20000], Loss: 0.4704\n",
      "Epoch [808/20000], Loss: 0.3456\n",
      "Epoch [809/20000], Loss: 0.3367\n",
      "Epoch [810/20000], Loss: 0.3450\n",
      "Epoch [811/20000], Loss: 0.4516\n",
      "Epoch [812/20000], Loss: 0.3599\n",
      "Epoch [813/20000], Loss: 0.3570\n",
      "Epoch [814/20000], Loss: 0.3732\n",
      "Epoch [815/20000], Loss: 0.3422\n",
      "Epoch [816/20000], Loss: 0.3897\n",
      "Epoch [817/20000], Loss: 0.3974\n",
      "Epoch [818/20000], Loss: 0.3788\n",
      "Epoch [819/20000], Loss: 0.3604\n",
      "Epoch [820/20000], Loss: 0.3607\n",
      "Epoch [821/20000], Loss: 0.3510\n",
      "Epoch [822/20000], Loss: 0.3925\n",
      "Epoch [823/20000], Loss: 0.4024\n",
      "Epoch [824/20000], Loss: 0.4696\n",
      "Epoch [825/20000], Loss: 0.4245\n",
      "Epoch [826/20000], Loss: 0.3689\n",
      "Epoch [827/20000], Loss: 0.4949\n",
      "Epoch [828/20000], Loss: 0.4090\n",
      "Epoch [829/20000], Loss: 0.4224\n",
      "Epoch [830/20000], Loss: 0.4229\n",
      "Epoch [831/20000], Loss: 0.3884\n",
      "Epoch [832/20000], Loss: 0.3825\n",
      "Epoch [833/20000], Loss: 0.3690\n",
      "Epoch [834/20000], Loss: 0.3967\n",
      "Epoch [835/20000], Loss: 0.4730\n",
      "Epoch [836/20000], Loss: 0.3285\n",
      "Epoch [837/20000], Loss: 0.4120\n",
      "Epoch [838/20000], Loss: 0.3388\n",
      "Epoch [839/20000], Loss: 0.4170\n",
      "Epoch [840/20000], Loss: 0.3933\n",
      "Epoch [841/20000], Loss: 0.3815\n",
      "Epoch [842/20000], Loss: 0.4324\n",
      "Epoch [843/20000], Loss: 0.3547\n",
      "Epoch [844/20000], Loss: 0.3424\n",
      "Epoch [845/20000], Loss: 0.4276\n",
      "Epoch [846/20000], Loss: 0.3704\n",
      "Epoch [847/20000], Loss: 0.3977\n",
      "Epoch [848/20000], Loss: 0.3306\n",
      "Epoch [849/20000], Loss: 0.3642\n",
      "Epoch [850/20000], Loss: 0.3797\n",
      "Epoch [851/20000], Loss: 0.4324\n",
      "Epoch [852/20000], Loss: 0.3821\n",
      "Epoch [853/20000], Loss: 0.3414\n",
      "Epoch [854/20000], Loss: 0.4367\n",
      "Epoch [855/20000], Loss: 0.3671\n",
      "Epoch [856/20000], Loss: 0.4497\n",
      "Epoch [857/20000], Loss: 0.4458\n",
      "Epoch [858/20000], Loss: 0.4362\n",
      "Epoch [859/20000], Loss: 0.4556\n",
      "Epoch [860/20000], Loss: 0.3536\n",
      "Epoch [861/20000], Loss: 0.4211\n",
      "Epoch [862/20000], Loss: 0.3300\n",
      "Epoch [863/20000], Loss: 0.3809\n",
      "Epoch [864/20000], Loss: 0.3723\n",
      "Epoch [865/20000], Loss: 0.4736\n",
      "Epoch [866/20000], Loss: 0.3603\n",
      "Epoch [867/20000], Loss: 0.3399\n",
      "Epoch [868/20000], Loss: 0.4016\n",
      "Epoch [869/20000], Loss: 0.3885\n",
      "Epoch [870/20000], Loss: 0.4804\n",
      "Epoch [871/20000], Loss: 0.3453\n",
      "Epoch [872/20000], Loss: 0.4217\n",
      "Epoch [873/20000], Loss: 0.3423\n",
      "Epoch [874/20000], Loss: 0.4134\n",
      "Epoch [875/20000], Loss: 0.3755\n",
      "Epoch [876/20000], Loss: 0.4167\n",
      "Epoch [877/20000], Loss: 0.4265\n",
      "Epoch [878/20000], Loss: 0.3259\n",
      "Epoch [879/20000], Loss: 0.3288\n",
      "Epoch [880/20000], Loss: 0.4850\n",
      "Epoch [881/20000], Loss: 0.3477\n",
      "Epoch [882/20000], Loss: 0.3203\n",
      "Epoch [883/20000], Loss: 0.3681\n",
      "Epoch [884/20000], Loss: 0.4031\n",
      "Epoch [885/20000], Loss: 0.4014\n",
      "Epoch [886/20000], Loss: 0.3410\n",
      "Epoch [887/20000], Loss: 0.5473\n",
      "Epoch [888/20000], Loss: 0.3749\n",
      "Epoch [889/20000], Loss: 0.3980\n",
      "Epoch [890/20000], Loss: 0.4223\n",
      "Epoch [891/20000], Loss: 0.3543\n",
      "Epoch [892/20000], Loss: 0.4986\n",
      "Epoch [893/20000], Loss: 0.3714\n",
      "Epoch [894/20000], Loss: 0.4716\n",
      "Epoch [895/20000], Loss: 0.3470\n",
      "Epoch [896/20000], Loss: 0.3429\n",
      "Epoch [897/20000], Loss: 0.4820\n",
      "Epoch [898/20000], Loss: 0.4643\n",
      "Epoch [899/20000], Loss: 0.4617\n",
      "Epoch [900/20000], Loss: 0.3403\n",
      "Epoch [901/20000], Loss: 0.4823\n",
      "Epoch [902/20000], Loss: 0.4839\n",
      "Epoch [903/20000], Loss: 0.3726\n",
      "Epoch [904/20000], Loss: 0.3634\n",
      "Epoch [905/20000], Loss: 0.3503\n",
      "Epoch [906/20000], Loss: 0.4900\n",
      "Epoch [907/20000], Loss: 0.4393\n",
      "Epoch [908/20000], Loss: 0.4290\n",
      "Epoch [909/20000], Loss: 0.3557\n",
      "Epoch [910/20000], Loss: 0.3469\n",
      "Epoch [911/20000], Loss: 0.3646\n",
      "Epoch [912/20000], Loss: 0.4263\n",
      "Epoch [913/20000], Loss: 0.4509\n",
      "Epoch [914/20000], Loss: 0.4150\n",
      "Epoch [915/20000], Loss: 0.3500\n",
      "Epoch [916/20000], Loss: 0.3936\n",
      "Epoch [917/20000], Loss: 0.3964\n",
      "Epoch [918/20000], Loss: 0.4014\n",
      "Epoch [919/20000], Loss: 0.4914\n",
      "Epoch [920/20000], Loss: 0.3749\n",
      "Epoch [921/20000], Loss: 0.5126\n",
      "Epoch [922/20000], Loss: 0.3715\n",
      "Epoch [923/20000], Loss: 0.3324\n",
      "Epoch [924/20000], Loss: 0.3929\n",
      "Epoch [925/20000], Loss: 0.4800\n",
      "Epoch [926/20000], Loss: 0.4191\n",
      "Epoch [927/20000], Loss: 0.3795\n",
      "Epoch [928/20000], Loss: 0.4194\n",
      "Epoch [929/20000], Loss: 0.3522\n",
      "Epoch [930/20000], Loss: 0.4284\n",
      "Epoch [931/20000], Loss: 0.4230\n",
      "Epoch [932/20000], Loss: 0.3620\n",
      "Epoch [933/20000], Loss: 0.3825\n",
      "Epoch [934/20000], Loss: 0.3880\n",
      "Epoch [935/20000], Loss: 0.3972\n",
      "Epoch [936/20000], Loss: 0.3575\n",
      "Epoch [937/20000], Loss: 0.3958\n",
      "Epoch [938/20000], Loss: 0.4383\n",
      "Epoch [939/20000], Loss: 0.5376\n",
      "Epoch [940/20000], Loss: 0.3830\n",
      "Epoch [941/20000], Loss: 0.3670\n",
      "Epoch [942/20000], Loss: 0.3326\n",
      "Epoch [943/20000], Loss: 0.4563\n",
      "Epoch [944/20000], Loss: 0.3571\n",
      "Epoch [945/20000], Loss: 0.4706\n",
      "Epoch [946/20000], Loss: 0.3636\n",
      "Epoch [947/20000], Loss: 0.3415\n",
      "Epoch [948/20000], Loss: 0.3664\n",
      "Epoch [949/20000], Loss: 0.6114\n",
      "Epoch [950/20000], Loss: 0.3374\n",
      "Epoch [951/20000], Loss: 0.3717\n",
      "Epoch [952/20000], Loss: 0.4833\n",
      "Epoch [953/20000], Loss: 0.5005\n",
      "Epoch [954/20000], Loss: 0.4604\n",
      "Epoch [955/20000], Loss: 0.3536\n",
      "Epoch [956/20000], Loss: 0.4028\n",
      "Epoch [957/20000], Loss: 0.5882\n",
      "Epoch [958/20000], Loss: 0.3474\n",
      "Epoch [959/20000], Loss: 0.3742\n",
      "Epoch [960/20000], Loss: 0.3513\n",
      "Epoch [961/20000], Loss: 0.4607\n",
      "Epoch [962/20000], Loss: 0.4178\n",
      "Epoch [963/20000], Loss: 0.3432\n",
      "Epoch [964/20000], Loss: 0.3631\n",
      "Epoch [965/20000], Loss: 0.3505\n",
      "Epoch [966/20000], Loss: 0.3486\n",
      "Epoch [967/20000], Loss: 0.4596\n",
      "Epoch [968/20000], Loss: 0.3587\n",
      "Epoch [969/20000], Loss: 0.4124\n",
      "Epoch [970/20000], Loss: 0.3544\n",
      "Epoch [971/20000], Loss: 0.4026\n",
      "Epoch [972/20000], Loss: 0.3154\n",
      "Epoch [973/20000], Loss: 0.4275\n",
      "Epoch [974/20000], Loss: 0.3208\n",
      "Epoch [975/20000], Loss: 0.3839\n",
      "Epoch [976/20000], Loss: 0.3849\n",
      "Epoch [977/20000], Loss: 0.3833\n",
      "Epoch [978/20000], Loss: 0.4030\n",
      "Epoch [979/20000], Loss: 0.3738\n",
      "Epoch [980/20000], Loss: 0.4738\n",
      "Epoch [981/20000], Loss: 0.4728\n",
      "Epoch [982/20000], Loss: 0.4503\n",
      "Epoch [983/20000], Loss: 0.4471\n",
      "Epoch [984/20000], Loss: 0.3469\n",
      "Epoch [985/20000], Loss: 0.4395\n",
      "Epoch [986/20000], Loss: 0.4448\n",
      "Epoch [987/20000], Loss: 0.4304\n",
      "Epoch [988/20000], Loss: 0.4784\n",
      "Epoch [989/20000], Loss: 0.3999\n",
      "Epoch [990/20000], Loss: 0.4049\n",
      "Epoch [991/20000], Loss: 0.3985\n",
      "Epoch [992/20000], Loss: 0.4090\n",
      "Epoch [993/20000], Loss: 0.4285\n",
      "Epoch [994/20000], Loss: 0.5076\n",
      "Epoch [995/20000], Loss: 0.3586\n",
      "Epoch [996/20000], Loss: 0.4451\n",
      "Epoch [997/20000], Loss: 0.4084\n",
      "Epoch [998/20000], Loss: 0.4268\n",
      "Epoch [999/20000], Loss: 0.4016\n",
      "Epoch [1000/20000], Loss: 0.3859\n",
      "Epoch [1001/20000], Loss: 0.3436\n",
      "Epoch [1002/20000], Loss: 0.4097\n",
      "Epoch [1003/20000], Loss: 0.3687\n",
      "Epoch [1004/20000], Loss: 0.3314\n",
      "Epoch [1005/20000], Loss: 0.4253\n",
      "Epoch [1006/20000], Loss: 0.3643\n",
      "Epoch [1007/20000], Loss: 0.4662\n",
      "Epoch [1008/20000], Loss: 0.3840\n",
      "Epoch [1009/20000], Loss: 0.4669\n",
      "Epoch [1010/20000], Loss: 0.4551\n",
      "Epoch [1011/20000], Loss: 0.3591\n",
      "Epoch [1012/20000], Loss: 0.3651\n",
      "Epoch [1013/20000], Loss: 0.3619\n",
      "Epoch [1014/20000], Loss: 0.3623\n",
      "Epoch [1015/20000], Loss: 0.3321\n",
      "Epoch [1016/20000], Loss: 0.5299\n",
      "Epoch [1017/20000], Loss: 0.3011\n",
      "Epoch [1018/20000], Loss: 0.4630\n",
      "Epoch [1019/20000], Loss: 0.4907\n",
      "Epoch [1020/20000], Loss: 0.3906\n",
      "Epoch [1021/20000], Loss: 0.3574\n",
      "Epoch [1022/20000], Loss: 0.4935\n",
      "Epoch [1023/20000], Loss: 0.5010\n",
      "Epoch [1024/20000], Loss: 0.3964\n",
      "Epoch [1025/20000], Loss: 0.4271\n",
      "Epoch [1026/20000], Loss: 0.4821\n",
      "Epoch [1027/20000], Loss: 0.5073\n",
      "Epoch [1028/20000], Loss: 0.4074\n",
      "Epoch [1029/20000], Loss: 0.4261\n",
      "Epoch [1030/20000], Loss: 0.3733\n",
      "Epoch [1031/20000], Loss: 0.4472\n",
      "Epoch [1032/20000], Loss: 0.5652\n",
      "Epoch [1033/20000], Loss: 0.3811\n",
      "Epoch [1034/20000], Loss: 0.3986\n",
      "Epoch [1035/20000], Loss: 0.3578\n",
      "Epoch [1036/20000], Loss: 0.3622\n",
      "Epoch [1037/20000], Loss: 0.3617\n",
      "Epoch [1038/20000], Loss: 0.3474\n",
      "Epoch [1039/20000], Loss: 0.3503\n",
      "Epoch [1040/20000], Loss: 0.3857\n",
      "Epoch [1041/20000], Loss: 0.4203\n",
      "Epoch [1042/20000], Loss: 0.4196\n",
      "Epoch [1043/20000], Loss: 0.4823\n",
      "Epoch [1044/20000], Loss: 0.3538\n",
      "Epoch [1045/20000], Loss: 0.3596\n",
      "Epoch [1046/20000], Loss: 0.4248\n",
      "Epoch [1047/20000], Loss: 0.3761\n",
      "Epoch [1048/20000], Loss: 0.3565\n",
      "Epoch [1049/20000], Loss: 0.3880\n",
      "Epoch [1050/20000], Loss: 0.4020\n",
      "Epoch [1051/20000], Loss: 0.4182\n",
      "Epoch [1052/20000], Loss: 0.4463\n",
      "Epoch [1053/20000], Loss: 0.4266\n",
      "Epoch [1054/20000], Loss: 0.4690\n",
      "Epoch [1055/20000], Loss: 0.3545\n",
      "Epoch [1056/20000], Loss: 0.4164\n",
      "Epoch [1057/20000], Loss: 0.4372\n",
      "Epoch [1058/20000], Loss: 0.4080\n",
      "Epoch [1059/20000], Loss: 0.4829\n",
      "Epoch [1060/20000], Loss: 0.4830\n",
      "Epoch [1061/20000], Loss: 0.4572\n",
      "Epoch [1062/20000], Loss: 0.3780\n",
      "Epoch [1063/20000], Loss: 0.4301\n",
      "Epoch [1064/20000], Loss: 0.4291\n",
      "Epoch [1065/20000], Loss: 0.3543\n",
      "Epoch [1066/20000], Loss: 0.3730\n",
      "Epoch [1067/20000], Loss: 0.3954\n",
      "Epoch [1068/20000], Loss: 0.4053\n",
      "Epoch [1069/20000], Loss: 0.3794\n",
      "Epoch [1070/20000], Loss: 0.3712\n",
      "Epoch [1071/20000], Loss: 0.3303\n",
      "Epoch [1072/20000], Loss: 0.3483\n",
      "Epoch [1073/20000], Loss: 0.3404\n",
      "Epoch [1074/20000], Loss: 0.3769\n",
      "Epoch [1075/20000], Loss: 0.5996\n",
      "Epoch [1076/20000], Loss: 0.3736\n",
      "Epoch [1077/20000], Loss: 0.3945\n",
      "Epoch [1078/20000], Loss: 0.4704\n",
      "Epoch [1079/20000], Loss: 0.4135\n",
      "Epoch [1080/20000], Loss: 0.3454\n",
      "Epoch [1081/20000], Loss: 0.3422\n",
      "Epoch [1082/20000], Loss: 0.4025\n",
      "Epoch [1083/20000], Loss: 0.3180\n",
      "Epoch [1084/20000], Loss: 0.3803\n",
      "Epoch [1085/20000], Loss: 0.4260\n",
      "Epoch [1086/20000], Loss: 0.3739\n",
      "Epoch [1087/20000], Loss: 0.4225\n",
      "Epoch [1088/20000], Loss: 0.3998\n",
      "Epoch [1089/20000], Loss: 0.3343\n",
      "Epoch [1090/20000], Loss: 0.5456\n",
      "Epoch [1091/20000], Loss: 0.3369\n",
      "Epoch [1092/20000], Loss: 0.3166\n",
      "Epoch [1093/20000], Loss: 0.3351\n",
      "Epoch [1094/20000], Loss: 0.3619\n",
      "Epoch [1095/20000], Loss: 0.3218\n",
      "Epoch [1096/20000], Loss: 0.4802\n",
      "Epoch [1097/20000], Loss: 0.3812\n",
      "Epoch [1098/20000], Loss: 0.3654\n",
      "Epoch [1099/20000], Loss: 0.5007\n",
      "Epoch [1100/20000], Loss: 0.3861\n",
      "Epoch [1101/20000], Loss: 0.3775\n",
      "Epoch [1102/20000], Loss: 0.3538\n",
      "Epoch [1103/20000], Loss: 0.3615\n",
      "Epoch [1104/20000], Loss: 0.3625\n",
      "Epoch [1105/20000], Loss: 0.3218\n",
      "Epoch [1106/20000], Loss: 0.3402\n",
      "Epoch [1107/20000], Loss: 0.3375\n",
      "Epoch [1108/20000], Loss: 0.4406\n",
      "Epoch [1109/20000], Loss: 0.3975\n",
      "Epoch [1110/20000], Loss: 0.3645\n",
      "Epoch [1111/20000], Loss: 0.3620\n",
      "Epoch [1112/20000], Loss: 0.3529\n",
      "Epoch [1113/20000], Loss: 0.3546\n",
      "Epoch [1114/20000], Loss: 0.3129\n",
      "Epoch [1115/20000], Loss: 0.3920\n",
      "Epoch [1116/20000], Loss: 0.4074\n",
      "Epoch [1117/20000], Loss: 0.3251\n",
      "Epoch [1118/20000], Loss: 0.4360\n",
      "Epoch [1119/20000], Loss: 0.4642\n",
      "Epoch [1120/20000], Loss: 0.3812\n",
      "Epoch [1121/20000], Loss: 0.3721\n",
      "Epoch [1122/20000], Loss: 0.5027\n",
      "Epoch [1123/20000], Loss: 0.4704\n",
      "Epoch [1124/20000], Loss: 0.3237\n",
      "Epoch [1125/20000], Loss: 0.4163\n",
      "Epoch [1126/20000], Loss: 0.3561\n",
      "Epoch [1127/20000], Loss: 0.4347\n",
      "Epoch [1128/20000], Loss: 0.3731\n",
      "Epoch [1129/20000], Loss: 0.4156\n",
      "Epoch [1130/20000], Loss: 0.3558\n",
      "Epoch [1131/20000], Loss: 0.3301\n",
      "Epoch [1132/20000], Loss: 0.3662\n",
      "Epoch [1133/20000], Loss: 0.4004\n",
      "Epoch [1134/20000], Loss: 0.4083\n",
      "Epoch [1135/20000], Loss: 0.4331\n",
      "Epoch [1136/20000], Loss: 0.4258\n",
      "Epoch [1137/20000], Loss: 0.3513\n",
      "Epoch [1138/20000], Loss: 0.3859\n",
      "Epoch [1139/20000], Loss: 0.3786\n",
      "Epoch [1140/20000], Loss: 0.3090\n",
      "Epoch [1141/20000], Loss: 0.3630\n",
      "Epoch [1142/20000], Loss: 0.4033\n",
      "Epoch [1143/20000], Loss: 0.4073\n",
      "Epoch [1144/20000], Loss: 0.3576\n",
      "Epoch [1145/20000], Loss: 0.3790\n",
      "Epoch [1146/20000], Loss: 0.3638\n",
      "Epoch [1147/20000], Loss: 0.3719\n",
      "Epoch [1148/20000], Loss: 0.4063\n",
      "Epoch [1149/20000], Loss: 0.4113\n",
      "Epoch [1150/20000], Loss: 0.4350\n",
      "Epoch [1151/20000], Loss: 0.3741\n",
      "Epoch [1152/20000], Loss: 0.4282\n",
      "Epoch [1153/20000], Loss: 0.4221\n",
      "Epoch [1154/20000], Loss: 0.4275\n",
      "Epoch [1155/20000], Loss: 0.5067\n",
      "Epoch [1156/20000], Loss: 0.3583\n",
      "Epoch [1157/20000], Loss: 0.3557\n",
      "Epoch [1158/20000], Loss: 0.4230\n",
      "Epoch [1159/20000], Loss: 0.4093\n",
      "Epoch [1160/20000], Loss: 0.3726\n",
      "Epoch [1161/20000], Loss: 0.3571\n",
      "Epoch [1162/20000], Loss: 0.3990\n",
      "Epoch [1163/20000], Loss: 0.3842\n",
      "Epoch [1164/20000], Loss: 0.3472\n",
      "Epoch [1165/20000], Loss: 0.4346\n",
      "Epoch [1166/20000], Loss: 0.3046\n",
      "Epoch [1167/20000], Loss: 0.4528\n",
      "Epoch [1168/20000], Loss: 0.3815\n",
      "Epoch [1169/20000], Loss: 0.3632\n",
      "Epoch [1170/20000], Loss: 0.3775\n",
      "Epoch [1171/20000], Loss: 0.3420\n",
      "Epoch [1172/20000], Loss: 0.5145\n",
      "Epoch [1173/20000], Loss: 0.3602\n",
      "Epoch [1174/20000], Loss: 0.4169\n",
      "Epoch [1175/20000], Loss: 0.3801\n",
      "Epoch [1176/20000], Loss: 0.3905\n",
      "Epoch [1177/20000], Loss: 0.4230\n",
      "Epoch [1178/20000], Loss: 0.3714\n",
      "Epoch [1179/20000], Loss: 0.4742\n",
      "Epoch [1180/20000], Loss: 0.3399\n",
      "Epoch [1181/20000], Loss: 0.5078\n",
      "Epoch [1182/20000], Loss: 0.3574\n",
      "Epoch [1183/20000], Loss: 0.3765\n",
      "Epoch [1184/20000], Loss: 0.5273\n",
      "Epoch [1185/20000], Loss: 0.3715\n",
      "Epoch [1186/20000], Loss: 0.4062\n",
      "Epoch [1187/20000], Loss: 0.3503\n",
      "Epoch [1188/20000], Loss: 0.3395\n",
      "Epoch [1189/20000], Loss: 0.3125\n",
      "Epoch [1190/20000], Loss: 0.3502\n",
      "Epoch [1191/20000], Loss: 0.4125\n",
      "Epoch [1192/20000], Loss: 0.4183\n",
      "Epoch [1193/20000], Loss: 0.3655\n",
      "Epoch [1194/20000], Loss: 0.3742\n",
      "Epoch [1195/20000], Loss: 0.3207\n",
      "Epoch [1196/20000], Loss: 0.3400\n",
      "Epoch [1197/20000], Loss: 0.3585\n",
      "Epoch [1198/20000], Loss: 0.3874\n",
      "Epoch [1199/20000], Loss: 0.3773\n",
      "Epoch [1200/20000], Loss: 0.4824\n",
      "Epoch [1201/20000], Loss: 0.4326\n",
      "Epoch [1202/20000], Loss: 0.4298\n",
      "Epoch [1203/20000], Loss: 0.3726\n",
      "Epoch [1204/20000], Loss: 0.4060\n",
      "Epoch [1205/20000], Loss: 0.5014\n",
      "Epoch [1206/20000], Loss: 0.4229\n",
      "Epoch [1207/20000], Loss: 0.4200\n",
      "Epoch [1208/20000], Loss: 0.3598\n",
      "Epoch [1209/20000], Loss: 0.3594\n",
      "Epoch [1210/20000], Loss: 0.5465\n",
      "Epoch [1211/20000], Loss: 0.3805\n",
      "Epoch [1212/20000], Loss: 0.3327\n",
      "Epoch [1213/20000], Loss: 0.4185\n",
      "Epoch [1214/20000], Loss: 0.3917\n",
      "Epoch [1215/20000], Loss: 0.3522\n",
      "Epoch [1216/20000], Loss: 0.3966\n",
      "Epoch [1217/20000], Loss: 0.3292\n",
      "Epoch [1218/20000], Loss: 0.3941\n",
      "Epoch [1219/20000], Loss: 0.3753\n",
      "Epoch [1220/20000], Loss: 0.3843\n",
      "Epoch [1221/20000], Loss: 0.3933\n",
      "Epoch [1222/20000], Loss: 0.4136\n",
      "Epoch [1223/20000], Loss: 0.3837\n",
      "Epoch [1224/20000], Loss: 0.4079\n",
      "Epoch [1225/20000], Loss: 0.5182\n",
      "Epoch [1226/20000], Loss: 0.4058\n",
      "Epoch [1227/20000], Loss: 0.3921\n",
      "Epoch [1228/20000], Loss: 0.3937\n",
      "Epoch [1229/20000], Loss: 0.4348\n",
      "Epoch [1230/20000], Loss: 0.3603\n",
      "Epoch [1231/20000], Loss: 0.3341\n",
      "Epoch [1232/20000], Loss: 0.3953\n",
      "Epoch [1233/20000], Loss: 0.4124\n",
      "Epoch [1234/20000], Loss: 0.3703\n",
      "Epoch [1235/20000], Loss: 0.3757\n",
      "Epoch [1236/20000], Loss: 0.4771\n",
      "Epoch [1237/20000], Loss: 0.3178\n",
      "Epoch [1238/20000], Loss: 0.5328\n",
      "Epoch [1239/20000], Loss: 0.3822\n",
      "Epoch [1240/20000], Loss: 0.4383\n",
      "Epoch [1241/20000], Loss: 0.4564\n",
      "Epoch [1242/20000], Loss: 0.3550\n",
      "Epoch [1243/20000], Loss: 0.3464\n",
      "Epoch [1244/20000], Loss: 0.3479\n",
      "Epoch [1245/20000], Loss: 0.4045\n",
      "Epoch [1246/20000], Loss: 0.3395\n",
      "Epoch [1247/20000], Loss: 0.4809\n",
      "Epoch [1248/20000], Loss: 0.4526\n",
      "Epoch [1249/20000], Loss: 0.4163\n",
      "Epoch [1250/20000], Loss: 0.3146\n",
      "Epoch [1251/20000], Loss: 0.3970\n",
      "Epoch [1252/20000], Loss: 0.3412\n",
      "Epoch [1253/20000], Loss: 0.4255\n",
      "Epoch [1254/20000], Loss: 0.3304\n",
      "Epoch [1255/20000], Loss: 0.3786\n",
      "Epoch [1256/20000], Loss: 0.3704\n",
      "Epoch [1257/20000], Loss: 0.5335\n",
      "Epoch [1258/20000], Loss: 0.4152\n",
      "Epoch [1259/20000], Loss: 0.4985\n",
      "Epoch [1260/20000], Loss: 0.3460\n",
      "Epoch [1261/20000], Loss: 0.3915\n",
      "Epoch [1262/20000], Loss: 0.4011\n",
      "Epoch [1263/20000], Loss: 0.3852\n",
      "Epoch [1264/20000], Loss: 0.4062\n",
      "Epoch [1265/20000], Loss: 0.4181\n",
      "Epoch [1266/20000], Loss: 0.4077\n",
      "Epoch [1267/20000], Loss: 0.3376\n",
      "Epoch [1268/20000], Loss: 0.3429\n",
      "Epoch [1269/20000], Loss: 0.3536\n",
      "Epoch [1270/20000], Loss: 0.5318\n",
      "Epoch [1271/20000], Loss: 0.3526\n",
      "Epoch [1272/20000], Loss: 0.4266\n",
      "Epoch [1273/20000], Loss: 0.3626\n",
      "Epoch [1274/20000], Loss: 0.4323\n",
      "Epoch [1275/20000], Loss: 0.4663\n",
      "Epoch [1276/20000], Loss: 0.4577\n",
      "Epoch [1277/20000], Loss: 0.4415\n",
      "Epoch [1278/20000], Loss: 0.3343\n",
      "Epoch [1279/20000], Loss: 0.3773\n",
      "Epoch [1280/20000], Loss: 0.3666\n",
      "Epoch [1281/20000], Loss: 0.4745\n",
      "Epoch [1282/20000], Loss: 0.3679\n",
      "Epoch [1283/20000], Loss: 0.3572\n",
      "Epoch [1284/20000], Loss: 0.4227\n",
      "Epoch [1285/20000], Loss: 0.4447\n",
      "Epoch [1286/20000], Loss: 0.4178\n",
      "Epoch [1287/20000], Loss: 0.3862\n",
      "Epoch [1288/20000], Loss: 0.3626\n",
      "Epoch [1289/20000], Loss: 0.3714\n",
      "Epoch [1290/20000], Loss: 0.4490\n",
      "Epoch [1291/20000], Loss: 0.3650\n",
      "Epoch [1292/20000], Loss: 0.4780\n",
      "Epoch [1293/20000], Loss: 0.3987\n",
      "Epoch [1294/20000], Loss: 0.4201\n",
      "Epoch [1295/20000], Loss: 0.4137\n",
      "Epoch [1296/20000], Loss: 0.3517\n",
      "Epoch [1297/20000], Loss: 0.2977\n",
      "Epoch [1298/20000], Loss: 0.4838\n",
      "Epoch [1299/20000], Loss: 0.4293\n",
      "Epoch [1300/20000], Loss: 0.4310\n",
      "Epoch [1301/20000], Loss: 0.3650\n",
      "Epoch [1302/20000], Loss: 0.3579\n",
      "Epoch [1303/20000], Loss: 0.4287\n",
      "Epoch [1304/20000], Loss: 0.3392\n",
      "Epoch [1305/20000], Loss: 0.4519\n",
      "Epoch [1306/20000], Loss: 0.3650\n",
      "Epoch [1307/20000], Loss: 0.4295\n",
      "Epoch [1308/20000], Loss: 0.4153\n",
      "Epoch [1309/20000], Loss: 0.3516\n",
      "Epoch [1310/20000], Loss: 0.4367\n",
      "Epoch [1311/20000], Loss: 0.4144\n",
      "Epoch [1312/20000], Loss: 0.4565\n",
      "Epoch [1313/20000], Loss: 0.3877\n",
      "Epoch [1314/20000], Loss: 0.4338\n",
      "Epoch [1315/20000], Loss: 0.3630\n",
      "Epoch [1316/20000], Loss: 0.5474\n",
      "Epoch [1317/20000], Loss: 0.3804\n",
      "Epoch [1318/20000], Loss: 0.3917\n",
      "Epoch [1319/20000], Loss: 0.3745\n",
      "Epoch [1320/20000], Loss: 0.3652\n",
      "Epoch [1321/20000], Loss: 0.3874\n",
      "Epoch [1322/20000], Loss: 0.5033\n",
      "Epoch [1323/20000], Loss: 0.4940\n",
      "Epoch [1324/20000], Loss: 0.3235\n",
      "Epoch [1325/20000], Loss: 0.4257\n",
      "Epoch [1326/20000], Loss: 0.3985\n",
      "Epoch [1327/20000], Loss: 0.4311\n",
      "Epoch [1328/20000], Loss: 0.4273\n",
      "Epoch [1329/20000], Loss: 0.3693\n",
      "Epoch [1330/20000], Loss: 0.3223\n",
      "Epoch [1331/20000], Loss: 0.4058\n",
      "Epoch [1332/20000], Loss: 0.3910\n",
      "Epoch [1333/20000], Loss: 0.3460\n",
      "Epoch [1334/20000], Loss: 0.4073\n",
      "Epoch [1335/20000], Loss: 0.4749\n",
      "Epoch [1336/20000], Loss: 0.3734\n",
      "Epoch [1337/20000], Loss: 0.5064\n",
      "Epoch [1338/20000], Loss: 0.3537\n",
      "Epoch [1339/20000], Loss: 0.3318\n",
      "Epoch [1340/20000], Loss: 0.4768\n",
      "Epoch [1341/20000], Loss: 0.3982\n",
      "Epoch [1342/20000], Loss: 0.4467\n",
      "Epoch [1343/20000], Loss: 0.4825\n",
      "Epoch [1344/20000], Loss: 0.3419\n",
      "Epoch [1345/20000], Loss: 0.4589\n",
      "Epoch [1346/20000], Loss: 0.3611\n",
      "Epoch [1347/20000], Loss: 0.3296\n",
      "Epoch [1348/20000], Loss: 0.4778\n",
      "Epoch [1349/20000], Loss: 0.3102\n",
      "Epoch [1350/20000], Loss: 0.4108\n",
      "Epoch [1351/20000], Loss: 0.3536\n",
      "Epoch [1352/20000], Loss: 0.3736\n",
      "Epoch [1353/20000], Loss: 0.4174\n",
      "Epoch [1354/20000], Loss: 0.3906\n",
      "Epoch [1355/20000], Loss: 0.4367\n",
      "Epoch [1356/20000], Loss: 0.3605\n",
      "Epoch [1357/20000], Loss: 0.3039\n",
      "Epoch [1358/20000], Loss: 0.3491\n",
      "Epoch [1359/20000], Loss: 0.3648\n",
      "Epoch [1360/20000], Loss: 0.3525\n",
      "Epoch [1361/20000], Loss: 0.3945\n",
      "Epoch [1362/20000], Loss: 0.4767\n",
      "Epoch [1363/20000], Loss: 0.4354\n",
      "Epoch [1364/20000], Loss: 0.4643\n",
      "Epoch [1365/20000], Loss: 0.4994\n",
      "Epoch [1366/20000], Loss: 0.5606\n",
      "Epoch [1367/20000], Loss: 0.4150\n",
      "Epoch [1368/20000], Loss: 0.4453\n",
      "Epoch [1369/20000], Loss: 0.3532\n",
      "Epoch [1370/20000], Loss: 0.3697\n",
      "Epoch [1371/20000], Loss: 0.4118\n",
      "Epoch [1372/20000], Loss: 0.4313\n",
      "Epoch [1373/20000], Loss: 0.3868\n",
      "Epoch [1374/20000], Loss: 0.3971\n",
      "Epoch [1375/20000], Loss: 0.3535\n",
      "Epoch [1376/20000], Loss: 0.3745\n",
      "Epoch [1377/20000], Loss: 0.3265\n",
      "Epoch [1378/20000], Loss: 0.3562\n",
      "Epoch [1379/20000], Loss: 0.3434\n",
      "Epoch [1380/20000], Loss: 0.4384\n",
      "Epoch [1381/20000], Loss: 0.4036\n",
      "Epoch [1382/20000], Loss: 0.4136\n",
      "Epoch [1383/20000], Loss: 0.4728\n",
      "Epoch [1384/20000], Loss: 0.3802\n",
      "Epoch [1385/20000], Loss: 0.5016\n",
      "Epoch [1386/20000], Loss: 0.3566\n",
      "Epoch [1387/20000], Loss: 0.4588\n",
      "Epoch [1388/20000], Loss: 0.3319\n",
      "Epoch [1389/20000], Loss: 0.4146\n",
      "Epoch [1390/20000], Loss: 0.4858\n",
      "Epoch [1391/20000], Loss: 0.4011\n",
      "Epoch [1392/20000], Loss: 0.3408\n",
      "Epoch [1393/20000], Loss: 0.3809\n",
      "Epoch [1394/20000], Loss: 0.3759\n",
      "Epoch [1395/20000], Loss: 0.3429\n",
      "Epoch [1396/20000], Loss: 0.3669\n",
      "Epoch [1397/20000], Loss: 0.4636\n",
      "Epoch [1398/20000], Loss: 0.4958\n",
      "Epoch [1399/20000], Loss: 0.4936\n",
      "Epoch [1400/20000], Loss: 0.4063\n",
      "Epoch [1401/20000], Loss: 0.3446\n",
      "Epoch [1402/20000], Loss: 0.3608\n",
      "Epoch [1403/20000], Loss: 0.3750\n",
      "Epoch [1404/20000], Loss: 0.3667\n",
      "Epoch [1405/20000], Loss: 0.4493\n",
      "Epoch [1406/20000], Loss: 0.3473\n",
      "Epoch [1407/20000], Loss: 0.3737\n",
      "Epoch [1408/20000], Loss: 0.3825\n",
      "Epoch [1409/20000], Loss: 0.4734\n",
      "Epoch [1410/20000], Loss: 0.4280\n",
      "Epoch [1411/20000], Loss: 0.3431\n",
      "Epoch [1412/20000], Loss: 0.3959\n",
      "Epoch [1413/20000], Loss: 0.5291\n",
      "Epoch [1414/20000], Loss: 0.3620\n",
      "Epoch [1415/20000], Loss: 0.4107\n",
      "Epoch [1416/20000], Loss: 0.4001\n",
      "Epoch [1417/20000], Loss: 0.4066\n",
      "Epoch [1418/20000], Loss: 0.4401\n",
      "Epoch [1419/20000], Loss: 0.3375\n",
      "Epoch [1420/20000], Loss: 0.3460\n",
      "Epoch [1421/20000], Loss: 0.4216\n",
      "Epoch [1422/20000], Loss: 0.5083\n",
      "Epoch [1423/20000], Loss: 0.3927\n",
      "Epoch [1424/20000], Loss: 0.5485\n",
      "Epoch [1425/20000], Loss: 0.3725\n",
      "Epoch [1426/20000], Loss: 0.3463\n",
      "Epoch [1427/20000], Loss: 0.3504\n",
      "Epoch [1428/20000], Loss: 0.3930\n",
      "Epoch [1429/20000], Loss: 0.4726\n",
      "Epoch [1430/20000], Loss: 0.4554\n",
      "Epoch [1431/20000], Loss: 0.3849\n",
      "Epoch [1432/20000], Loss: 0.3730\n",
      "Epoch [1433/20000], Loss: 0.4332\n",
      "Epoch [1434/20000], Loss: 0.4636\n",
      "Epoch [1435/20000], Loss: 0.3739\n",
      "Epoch [1436/20000], Loss: 0.3523\n",
      "Epoch [1437/20000], Loss: 0.3730\n",
      "Epoch [1438/20000], Loss: 0.3311\n",
      "Epoch [1439/20000], Loss: 0.3526\n",
      "Epoch [1440/20000], Loss: 0.6147\n",
      "Epoch [1441/20000], Loss: 0.3379\n",
      "Epoch [1442/20000], Loss: 0.3488\n",
      "Epoch [1443/20000], Loss: 0.3603\n",
      "Epoch [1444/20000], Loss: 0.5204\n",
      "Epoch [1445/20000], Loss: 0.3467\n",
      "Epoch [1446/20000], Loss: 0.3270\n",
      "Epoch [1447/20000], Loss: 0.4542\n",
      "Epoch [1448/20000], Loss: 0.3696\n",
      "Epoch [1449/20000], Loss: 0.3685\n",
      "Epoch [1450/20000], Loss: 0.3870\n",
      "Epoch [1451/20000], Loss: 0.4334\n",
      "Epoch [1452/20000], Loss: 0.3355\n",
      "Epoch [1453/20000], Loss: 0.4439\n",
      "Epoch [1454/20000], Loss: 0.3735\n",
      "Epoch [1455/20000], Loss: 0.3621\n",
      "Epoch [1456/20000], Loss: 0.3494\n",
      "Epoch [1457/20000], Loss: 0.3963\n",
      "Epoch [1458/20000], Loss: 0.4458\n",
      "Epoch [1459/20000], Loss: 0.4456\n",
      "Epoch [1460/20000], Loss: 0.4489\n",
      "Epoch [1461/20000], Loss: 0.3404\n",
      "Epoch [1462/20000], Loss: 0.3621\n",
      "Epoch [1463/20000], Loss: 0.5052\n",
      "Epoch [1464/20000], Loss: 0.4542\n",
      "Epoch [1465/20000], Loss: 0.3287\n",
      "Epoch [1466/20000], Loss: 0.4377\n",
      "Epoch [1467/20000], Loss: 0.4080\n",
      "Epoch [1468/20000], Loss: 0.3665\n",
      "Epoch [1469/20000], Loss: 0.4188\n",
      "Epoch [1470/20000], Loss: 0.3986\n",
      "Epoch [1471/20000], Loss: 0.3502\n",
      "Epoch [1472/20000], Loss: 0.3492\n",
      "Epoch [1473/20000], Loss: 0.4214\n",
      "Epoch [1474/20000], Loss: 0.3505\n",
      "Epoch [1475/20000], Loss: 0.4023\n",
      "Epoch [1476/20000], Loss: 0.4232\n",
      "Epoch [1477/20000], Loss: 0.3259\n",
      "Epoch [1478/20000], Loss: 0.4113\n",
      "Epoch [1479/20000], Loss: 0.4630\n",
      "Epoch [1480/20000], Loss: 0.3610\n",
      "Epoch [1481/20000], Loss: 0.3743\n",
      "Epoch [1482/20000], Loss: 0.3660\n",
      "Epoch [1483/20000], Loss: 0.4540\n",
      "Epoch [1484/20000], Loss: 0.4272\n",
      "Epoch [1485/20000], Loss: 0.3688\n",
      "Epoch [1486/20000], Loss: 0.4309\n",
      "Epoch [1487/20000], Loss: 0.4779\n",
      "Epoch [1488/20000], Loss: 0.4480\n",
      "Epoch [1489/20000], Loss: 0.4169\n",
      "Epoch [1490/20000], Loss: 0.4064\n",
      "Epoch [1491/20000], Loss: 0.3871\n",
      "Epoch [1492/20000], Loss: 0.4318\n",
      "Epoch [1493/20000], Loss: 0.3422\n",
      "Epoch [1494/20000], Loss: 0.4231\n",
      "Epoch [1495/20000], Loss: 0.4008\n",
      "Epoch [1496/20000], Loss: 0.4111\n",
      "Epoch [1497/20000], Loss: 0.3966\n",
      "Epoch [1498/20000], Loss: 0.3454\n",
      "Epoch [1499/20000], Loss: 0.3933\n",
      "Epoch [1500/20000], Loss: 0.3393\n",
      "Epoch [1501/20000], Loss: 0.3713\n",
      "Epoch [1502/20000], Loss: 0.3282\n",
      "Epoch [1503/20000], Loss: 0.3151\n",
      "Epoch [1504/20000], Loss: 0.3588\n",
      "Epoch [1505/20000], Loss: 0.3902\n",
      "Epoch [1506/20000], Loss: 0.4444\n",
      "Epoch [1507/20000], Loss: 0.3639\n",
      "Epoch [1508/20000], Loss: 0.3516\n",
      "Epoch [1509/20000], Loss: 0.3843\n",
      "Epoch [1510/20000], Loss: 0.4144\n",
      "Epoch [1511/20000], Loss: 0.3312\n",
      "Epoch [1512/20000], Loss: 0.3358\n",
      "Epoch [1513/20000], Loss: 0.3628\n",
      "Epoch [1514/20000], Loss: 0.3376\n",
      "Epoch [1515/20000], Loss: 0.3480\n",
      "Epoch [1516/20000], Loss: 0.3125\n",
      "Epoch [1517/20000], Loss: 0.3895\n",
      "Epoch [1518/20000], Loss: 0.3263\n",
      "Epoch [1519/20000], Loss: 0.3700\n",
      "Epoch [1520/20000], Loss: 0.3347\n",
      "Epoch [1521/20000], Loss: 0.4046\n",
      "Epoch [1522/20000], Loss: 0.4675\n",
      "Epoch [1523/20000], Loss: 0.4189\n",
      "Epoch [1524/20000], Loss: 0.4111\n",
      "Epoch [1525/20000], Loss: 0.4246\n",
      "Epoch [1526/20000], Loss: 0.3934\n",
      "Epoch [1527/20000], Loss: 0.3905\n",
      "Epoch [1528/20000], Loss: 0.3817\n",
      "Epoch [1529/20000], Loss: 0.4381\n",
      "Epoch [1530/20000], Loss: 0.4076\n",
      "Epoch [1531/20000], Loss: 0.3348\n",
      "Epoch [1532/20000], Loss: 0.3245\n",
      "Epoch [1533/20000], Loss: 0.4750\n",
      "Epoch [1534/20000], Loss: 0.3825\n",
      "Epoch [1535/20000], Loss: 0.3383\n",
      "Epoch [1536/20000], Loss: 0.4366\n",
      "Epoch [1537/20000], Loss: 0.5057\n",
      "Epoch [1538/20000], Loss: 0.3362\n",
      "Epoch [1539/20000], Loss: 0.4903\n",
      "Epoch [1540/20000], Loss: 0.3702\n",
      "Epoch [1541/20000], Loss: 0.4749\n",
      "Epoch [1542/20000], Loss: 0.4309\n",
      "Epoch [1543/20000], Loss: 0.3300\n",
      "Epoch [1544/20000], Loss: 0.3518\n",
      "Epoch [1545/20000], Loss: 0.4987\n",
      "Epoch [1546/20000], Loss: 0.5426\n",
      "Epoch [1547/20000], Loss: 0.4164\n",
      "Epoch [1548/20000], Loss: 0.3453\n",
      "Epoch [1549/20000], Loss: 0.3654\n",
      "Epoch [1550/20000], Loss: 0.4155\n",
      "Epoch [1551/20000], Loss: 0.4225\n",
      "Epoch [1552/20000], Loss: 0.3176\n",
      "Epoch [1553/20000], Loss: 0.3268\n",
      "Epoch [1554/20000], Loss: 0.3418\n",
      "Epoch [1555/20000], Loss: 0.3702\n",
      "Epoch [1556/20000], Loss: 0.3403\n",
      "Epoch [1557/20000], Loss: 0.3757\n",
      "Epoch [1558/20000], Loss: 0.3623\n",
      "Epoch [1559/20000], Loss: 0.3562\n",
      "Epoch [1560/20000], Loss: 0.4502\n",
      "Epoch [1561/20000], Loss: 0.3705\n",
      "Epoch [1562/20000], Loss: 0.3785\n",
      "Epoch [1563/20000], Loss: 0.4219\n",
      "Epoch [1564/20000], Loss: 0.3694\n",
      "Epoch [1565/20000], Loss: 0.4163\n",
      "Epoch [1566/20000], Loss: 0.3370\n",
      "Epoch [1567/20000], Loss: 0.3350\n",
      "Epoch [1568/20000], Loss: 0.3412\n",
      "Epoch [1569/20000], Loss: 0.3509\n",
      "Epoch [1570/20000], Loss: 0.3407\n",
      "Epoch [1571/20000], Loss: 0.4317\n",
      "Epoch [1572/20000], Loss: 0.3845\n",
      "Epoch [1573/20000], Loss: 0.3867\n",
      "Epoch [1574/20000], Loss: 0.3392\n",
      "Epoch [1575/20000], Loss: 0.3677\n",
      "Epoch [1576/20000], Loss: 0.3827\n",
      "Epoch [1577/20000], Loss: 0.4695\n",
      "Epoch [1578/20000], Loss: 0.3203\n",
      "Epoch [1579/20000], Loss: 0.3191\n",
      "Epoch [1580/20000], Loss: 0.4343\n",
      "Epoch [1581/20000], Loss: 0.3678\n",
      "Epoch [1582/20000], Loss: 0.5382\n",
      "Epoch [1583/20000], Loss: 0.3522\n",
      "Epoch [1584/20000], Loss: 0.3442\n",
      "Epoch [1585/20000], Loss: 0.3814\n",
      "Epoch [1586/20000], Loss: 0.3668\n",
      "Epoch [1587/20000], Loss: 0.4184\n",
      "Epoch [1588/20000], Loss: 0.4151\n",
      "Epoch [1589/20000], Loss: 0.3904\n",
      "Epoch [1590/20000], Loss: 0.3615\n",
      "Epoch [1591/20000], Loss: 0.4125\n",
      "Epoch [1592/20000], Loss: 0.3774\n",
      "Epoch [1593/20000], Loss: 0.3425\n",
      "Epoch [1594/20000], Loss: 0.3428\n",
      "Epoch [1595/20000], Loss: 0.3953\n",
      "Epoch [1596/20000], Loss: 0.4012\n",
      "Epoch [1597/20000], Loss: 0.4989\n",
      "Epoch [1598/20000], Loss: 0.3555\n",
      "Epoch [1599/20000], Loss: 0.4960\n",
      "Epoch [1600/20000], Loss: 0.5424\n",
      "Epoch [1601/20000], Loss: 0.4137\n",
      "Epoch [1602/20000], Loss: 0.4275\n",
      "Epoch [1603/20000], Loss: 0.3595\n",
      "Epoch [1604/20000], Loss: 0.3736\n",
      "Epoch [1605/20000], Loss: 0.4101\n",
      "Epoch [1606/20000], Loss: 0.5142\n",
      "Epoch [1607/20000], Loss: 0.3567\n",
      "Epoch [1608/20000], Loss: 0.3712\n",
      "Epoch [1609/20000], Loss: 0.4535\n",
      "Epoch [1610/20000], Loss: 0.3252\n",
      "Epoch [1611/20000], Loss: 0.3469\n",
      "Epoch [1612/20000], Loss: 0.3178\n",
      "Epoch [1613/20000], Loss: 0.3474\n",
      "Epoch [1614/20000], Loss: 0.3722\n",
      "Epoch [1615/20000], Loss: 0.3768\n",
      "Epoch [1616/20000], Loss: 0.4014\n",
      "Epoch [1617/20000], Loss: 0.4480\n",
      "Epoch [1618/20000], Loss: 0.4502\n",
      "Epoch [1619/20000], Loss: 0.3784\n",
      "Epoch [1620/20000], Loss: 0.4020\n",
      "Epoch [1621/20000], Loss: 0.4537\n",
      "Epoch [1622/20000], Loss: 0.3759\n",
      "Epoch [1623/20000], Loss: 0.4271\n",
      "Epoch [1624/20000], Loss: 0.3416\n",
      "Epoch [1625/20000], Loss: 0.4202\n",
      "Epoch [1626/20000], Loss: 0.3679\n",
      "Epoch [1627/20000], Loss: 0.3118\n",
      "Epoch [1628/20000], Loss: 0.3788\n",
      "Epoch [1629/20000], Loss: 0.3409\n",
      "Epoch [1630/20000], Loss: 0.4963\n",
      "Epoch [1631/20000], Loss: 0.4162\n",
      "Epoch [1632/20000], Loss: 0.3949\n",
      "Epoch [1633/20000], Loss: 0.3850\n",
      "Epoch [1634/20000], Loss: 0.4692\n",
      "Epoch [1635/20000], Loss: 0.4375\n",
      "Epoch [1636/20000], Loss: 0.3872\n",
      "Epoch [1637/20000], Loss: 0.3793\n",
      "Epoch [1638/20000], Loss: 0.4727\n",
      "Epoch [1639/20000], Loss: 0.3818\n",
      "Epoch [1640/20000], Loss: 0.3691\n",
      "Epoch [1641/20000], Loss: 0.4881\n",
      "Epoch [1642/20000], Loss: 0.3454\n",
      "Epoch [1643/20000], Loss: 0.4785\n",
      "Epoch [1644/20000], Loss: 0.3277\n",
      "Epoch [1645/20000], Loss: 0.3200\n",
      "Epoch [1646/20000], Loss: 0.4166\n",
      "Epoch [1647/20000], Loss: 0.5399\n",
      "Epoch [1648/20000], Loss: 0.3593\n",
      "Epoch [1649/20000], Loss: 0.3647\n",
      "Epoch [1650/20000], Loss: 0.4459\n",
      "Epoch [1651/20000], Loss: 0.3368\n",
      "Epoch [1652/20000], Loss: 0.4098\n",
      "Epoch [1653/20000], Loss: 0.5725\n",
      "Epoch [1654/20000], Loss: 0.3900\n",
      "Epoch [1655/20000], Loss: 0.3756\n",
      "Epoch [1656/20000], Loss: 0.3958\n",
      "Epoch [1657/20000], Loss: 0.3548\n",
      "Epoch [1658/20000], Loss: 0.4234\n",
      "Epoch [1659/20000], Loss: 0.3618\n",
      "Epoch [1660/20000], Loss: 0.3671\n",
      "Epoch [1661/20000], Loss: 0.3912\n",
      "Epoch [1662/20000], Loss: 0.4823\n",
      "Epoch [1663/20000], Loss: 0.3773\n",
      "Epoch [1664/20000], Loss: 0.3149\n",
      "Epoch [1665/20000], Loss: 0.3890\n",
      "Epoch [1666/20000], Loss: 0.3653\n",
      "Epoch [1667/20000], Loss: 0.4920\n",
      "Epoch [1668/20000], Loss: 0.3556\n",
      "Epoch [1669/20000], Loss: 0.3142\n",
      "Epoch [1670/20000], Loss: 0.4775\n",
      "Epoch [1671/20000], Loss: 0.4227\n",
      "Epoch [1672/20000], Loss: 0.3508\n",
      "Epoch [1673/20000], Loss: 0.3564\n",
      "Epoch [1674/20000], Loss: 0.3848\n",
      "Epoch [1675/20000], Loss: 0.4615\n",
      "Epoch [1676/20000], Loss: 0.3889\n",
      "Epoch [1677/20000], Loss: 0.3844\n",
      "Epoch [1678/20000], Loss: 0.3970\n",
      "Epoch [1679/20000], Loss: 0.4066\n",
      "Epoch [1680/20000], Loss: 0.4294\n",
      "Epoch [1681/20000], Loss: 0.4491\n",
      "Epoch [1682/20000], Loss: 0.3503\n",
      "Epoch [1683/20000], Loss: 0.4186\n",
      "Epoch [1684/20000], Loss: 0.3501\n",
      "Epoch [1685/20000], Loss: 0.3588\n",
      "Epoch [1686/20000], Loss: 0.3739\n",
      "Epoch [1687/20000], Loss: 0.3706\n",
      "Epoch [1688/20000], Loss: 0.4463\n",
      "Epoch [1689/20000], Loss: 0.3872\n",
      "Epoch [1690/20000], Loss: 0.3409\n",
      "Epoch [1691/20000], Loss: 0.4629\n",
      "Epoch [1692/20000], Loss: 0.3634\n",
      "Epoch [1693/20000], Loss: 0.4361\n",
      "Epoch [1694/20000], Loss: 0.4274\n",
      "Epoch [1695/20000], Loss: 0.4504\n",
      "Epoch [1696/20000], Loss: 0.3648\n",
      "Epoch [1697/20000], Loss: 0.3888\n",
      "Epoch [1698/20000], Loss: 0.3436\n",
      "Epoch [1699/20000], Loss: 0.3874\n",
      "Epoch [1700/20000], Loss: 0.3704\n",
      "Epoch [1701/20000], Loss: 0.3412\n",
      "Epoch [1702/20000], Loss: 0.4445\n",
      "Epoch [1703/20000], Loss: 0.4644\n",
      "Epoch [1704/20000], Loss: 0.4797\n",
      "Epoch [1705/20000], Loss: 0.4418\n",
      "Epoch [1706/20000], Loss: 0.3162\n",
      "Epoch [1707/20000], Loss: 0.3702\n",
      "Epoch [1708/20000], Loss: 0.4504\n",
      "Epoch [1709/20000], Loss: 0.4618\n",
      "Epoch [1710/20000], Loss: 0.3483\n",
      "Epoch [1711/20000], Loss: 0.3527\n",
      "Epoch [1712/20000], Loss: 0.3139\n",
      "Epoch [1713/20000], Loss: 0.3591\n",
      "Epoch [1714/20000], Loss: 0.3537\n",
      "Epoch [1715/20000], Loss: 0.3950\n",
      "Epoch [1716/20000], Loss: 0.3760\n",
      "Epoch [1717/20000], Loss: 0.3514\n",
      "Epoch [1718/20000], Loss: 0.4144\n",
      "Epoch [1719/20000], Loss: 0.4112\n",
      "Epoch [1720/20000], Loss: 0.3278\n",
      "Epoch [1721/20000], Loss: 0.3693\n",
      "Epoch [1722/20000], Loss: 0.3814\n",
      "Epoch [1723/20000], Loss: 0.4390\n",
      "Epoch [1724/20000], Loss: 0.4787\n",
      "Epoch [1725/20000], Loss: 0.4506\n",
      "Epoch [1726/20000], Loss: 0.3741\n",
      "Epoch [1727/20000], Loss: 0.4954\n",
      "Epoch [1728/20000], Loss: 0.4437\n",
      "Epoch [1729/20000], Loss: 0.4247\n",
      "Epoch [1730/20000], Loss: 0.4525\n",
      "Epoch [1731/20000], Loss: 0.4074\n",
      "Epoch [1732/20000], Loss: 0.3413\n",
      "Epoch [1733/20000], Loss: 0.3419\n",
      "Epoch [1734/20000], Loss: 0.3892\n",
      "Epoch [1735/20000], Loss: 0.4666\n",
      "Epoch [1736/20000], Loss: 0.4313\n",
      "Epoch [1737/20000], Loss: 0.3625\n",
      "Epoch [1738/20000], Loss: 0.3657\n",
      "Epoch [1739/20000], Loss: 0.3708\n",
      "Epoch [1740/20000], Loss: 0.3567\n",
      "Epoch [1741/20000], Loss: 0.4161\n",
      "Epoch [1742/20000], Loss: 0.4101\n",
      "Epoch [1743/20000], Loss: 0.3490\n",
      "Epoch [1744/20000], Loss: 0.4140\n",
      "Epoch [1745/20000], Loss: 0.3638\n",
      "Epoch [1746/20000], Loss: 0.4206\n",
      "Epoch [1747/20000], Loss: 0.4991\n",
      "Epoch [1748/20000], Loss: 0.4035\n",
      "Epoch [1749/20000], Loss: 0.3210\n",
      "Epoch [1750/20000], Loss: 0.4852\n",
      "Epoch [1751/20000], Loss: 0.3273\n",
      "Epoch [1752/20000], Loss: 0.4118\n",
      "Epoch [1753/20000], Loss: 0.4598\n",
      "Epoch [1754/20000], Loss: 0.4136\n",
      "Epoch [1755/20000], Loss: 0.4088\n",
      "Epoch [1756/20000], Loss: 0.3869\n",
      "Epoch [1757/20000], Loss: 0.3839\n",
      "Epoch [1758/20000], Loss: 0.3274\n",
      "Epoch [1759/20000], Loss: 0.4093\n",
      "Epoch [1760/20000], Loss: 0.5119\n",
      "Epoch [1761/20000], Loss: 0.3366\n",
      "Epoch [1762/20000], Loss: 0.3664\n",
      "Epoch [1763/20000], Loss: 0.5597\n",
      "Epoch [1764/20000], Loss: 0.4051\n",
      "Epoch [1765/20000], Loss: 0.2921\n",
      "Epoch [1766/20000], Loss: 0.3918\n",
      "Epoch [1767/20000], Loss: 0.4082\n",
      "Epoch [1768/20000], Loss: 0.3674\n",
      "Epoch [1769/20000], Loss: 0.3318\n",
      "Epoch [1770/20000], Loss: 0.4896\n",
      "Epoch [1771/20000], Loss: 0.4144\n",
      "Epoch [1772/20000], Loss: 0.5443\n",
      "Epoch [1773/20000], Loss: 0.4321\n",
      "Epoch [1774/20000], Loss: 0.3871\n",
      "Epoch [1775/20000], Loss: 0.4459\n",
      "Epoch [1776/20000], Loss: 0.4110\n",
      "Epoch [1777/20000], Loss: 0.5669\n",
      "Epoch [1778/20000], Loss: 0.3652\n",
      "Epoch [1779/20000], Loss: 0.4983\n",
      "Epoch [1780/20000], Loss: 0.3441\n",
      "Epoch [1781/20000], Loss: 0.4741\n",
      "Epoch [1782/20000], Loss: 0.4999\n",
      "Epoch [1783/20000], Loss: 0.3729\n",
      "Epoch [1784/20000], Loss: 0.4073\n",
      "Epoch [1785/20000], Loss: 0.3848\n",
      "Epoch [1786/20000], Loss: 0.3352\n",
      "Epoch [1787/20000], Loss: 0.4154\n",
      "Epoch [1788/20000], Loss: 0.4818\n",
      "Epoch [1789/20000], Loss: 0.3330\n",
      "Epoch [1790/20000], Loss: 0.3432\n",
      "Epoch [1791/20000], Loss: 0.3611\n",
      "Epoch [1792/20000], Loss: 0.3510\n",
      "Epoch [1793/20000], Loss: 0.4114\n",
      "Epoch [1794/20000], Loss: 0.4092\n",
      "Epoch [1795/20000], Loss: 0.4742\n",
      "Epoch [1796/20000], Loss: 0.3707\n",
      "Epoch [1797/20000], Loss: 0.3597\n",
      "Epoch [1798/20000], Loss: 0.3667\n",
      "Epoch [1799/20000], Loss: 0.4255\n",
      "Epoch [1800/20000], Loss: 0.3357\n",
      "Epoch [1801/20000], Loss: 0.4151\n",
      "Epoch [1802/20000], Loss: 0.3855\n",
      "Epoch [1803/20000], Loss: 0.3734\n",
      "Epoch [1804/20000], Loss: 0.4266\n",
      "Epoch [1805/20000], Loss: 0.3795\n",
      "Epoch [1806/20000], Loss: 0.4188\n",
      "Epoch [1807/20000], Loss: 0.4215\n",
      "Epoch [1808/20000], Loss: 0.3425\n",
      "Epoch [1809/20000], Loss: 0.3789\n",
      "Epoch [1810/20000], Loss: 0.3268\n",
      "Epoch [1811/20000], Loss: 0.3680\n",
      "Epoch [1812/20000], Loss: 0.3459\n",
      "Epoch [1813/20000], Loss: 0.3375\n",
      "Epoch [1814/20000], Loss: 0.3828\n",
      "Epoch [1815/20000], Loss: 0.3825\n",
      "Epoch [1816/20000], Loss: 0.3404\n",
      "Epoch [1817/20000], Loss: 0.3343\n",
      "Epoch [1818/20000], Loss: 0.4282\n",
      "Epoch [1819/20000], Loss: 0.4131\n",
      "Epoch [1820/20000], Loss: 0.4199\n",
      "Epoch [1821/20000], Loss: 0.4647\n",
      "Epoch [1822/20000], Loss: 0.4995\n",
      "Epoch [1823/20000], Loss: 0.3832\n",
      "Epoch [1824/20000], Loss: 0.3609\n",
      "Epoch [1825/20000], Loss: 0.3552\n",
      "Epoch [1826/20000], Loss: 0.3670\n",
      "Epoch [1827/20000], Loss: 0.4292\n",
      "Epoch [1828/20000], Loss: 0.3518\n",
      "Epoch [1829/20000], Loss: 0.3319\n",
      "Epoch [1830/20000], Loss: 0.3630\n",
      "Epoch [1831/20000], Loss: 0.4342\n",
      "Epoch [1832/20000], Loss: 0.3363\n",
      "Epoch [1833/20000], Loss: 0.3401\n",
      "Epoch [1834/20000], Loss: 0.4060\n",
      "Epoch [1835/20000], Loss: 0.3574\n",
      "Epoch [1836/20000], Loss: 0.3402\n",
      "Epoch [1837/20000], Loss: 0.3958\n",
      "Epoch [1838/20000], Loss: 0.3112\n",
      "Epoch [1839/20000], Loss: 0.4060\n",
      "Epoch [1840/20000], Loss: 0.4324\n",
      "Epoch [1841/20000], Loss: 0.4440\n",
      "Epoch [1842/20000], Loss: 0.3323\n",
      "Epoch [1843/20000], Loss: 0.4642\n",
      "Epoch [1844/20000], Loss: 0.3836\n",
      "Epoch [1845/20000], Loss: 0.3936\n",
      "Epoch [1846/20000], Loss: 0.3844\n",
      "Epoch [1847/20000], Loss: 0.3629\n",
      "Epoch [1848/20000], Loss: 0.4293\n",
      "Epoch [1849/20000], Loss: 0.4601\n",
      "Epoch [1850/20000], Loss: 0.4035\n",
      "Epoch [1851/20000], Loss: 0.4254\n",
      "Epoch [1852/20000], Loss: 0.3844\n",
      "Epoch [1853/20000], Loss: 0.4663\n",
      "Epoch [1854/20000], Loss: 0.4891\n",
      "Epoch [1855/20000], Loss: 0.3608\n",
      "Epoch [1856/20000], Loss: 0.3431\n",
      "Epoch [1857/20000], Loss: 0.3791\n",
      "Epoch [1858/20000], Loss: 0.3835\n",
      "Epoch [1859/20000], Loss: 0.3417\n",
      "Epoch [1860/20000], Loss: 0.4468\n",
      "Epoch [1861/20000], Loss: 0.3949\n",
      "Epoch [1862/20000], Loss: 0.3367\n",
      "Epoch [1863/20000], Loss: 0.3943\n",
      "Epoch [1864/20000], Loss: 0.3204\n",
      "Epoch [1865/20000], Loss: 0.3476\n",
      "Epoch [1866/20000], Loss: 0.4162\n",
      "Epoch [1867/20000], Loss: 0.3600\n",
      "Epoch [1868/20000], Loss: 0.5175\n",
      "Epoch [1869/20000], Loss: 0.3797\n",
      "Epoch [1870/20000], Loss: 0.3688\n",
      "Epoch [1871/20000], Loss: 0.4190\n",
      "Epoch [1872/20000], Loss: 0.4345\n",
      "Epoch [1873/20000], Loss: 0.3222\n",
      "Epoch [1874/20000], Loss: 0.3065\n",
      "Epoch [1875/20000], Loss: 0.3707\n",
      "Epoch [1876/20000], Loss: 0.3789\n",
      "Epoch [1877/20000], Loss: 0.5987\n",
      "Epoch [1878/20000], Loss: 0.3659\n",
      "Epoch [1879/20000], Loss: 0.3832\n",
      "Epoch [1880/20000], Loss: 0.3629\n",
      "Epoch [1881/20000], Loss: 0.3962\n",
      "Epoch [1882/20000], Loss: 0.3075\n",
      "Epoch [1883/20000], Loss: 0.3922\n",
      "Epoch [1884/20000], Loss: 0.3387\n",
      "Epoch [1885/20000], Loss: 0.4216\n",
      "Epoch [1886/20000], Loss: 0.4660\n",
      "Epoch [1887/20000], Loss: 0.4525\n",
      "Epoch [1888/20000], Loss: 0.3819\n",
      "Epoch [1889/20000], Loss: 0.3891\n",
      "Epoch [1890/20000], Loss: 0.3765\n",
      "Epoch [1891/20000], Loss: 0.5002\n",
      "Epoch [1892/20000], Loss: 0.3458\n",
      "Epoch [1893/20000], Loss: 0.4575\n",
      "Epoch [1894/20000], Loss: 0.3720\n",
      "Epoch [1895/20000], Loss: 0.3609\n",
      "Epoch [1896/20000], Loss: 0.4713\n",
      "Epoch [1897/20000], Loss: 0.3755\n",
      "Epoch [1898/20000], Loss: 0.4462\n",
      "Epoch [1899/20000], Loss: 0.5073\n",
      "Epoch [1900/20000], Loss: 0.3779\n",
      "Epoch [1901/20000], Loss: 0.3354\n",
      "Epoch [1902/20000], Loss: 0.4457\n",
      "Epoch [1903/20000], Loss: 0.3640\n",
      "Epoch [1904/20000], Loss: 0.3333\n",
      "Epoch [1905/20000], Loss: 0.3887\n",
      "Epoch [1906/20000], Loss: 0.3774\n",
      "Epoch [1907/20000], Loss: 0.3938\n",
      "Epoch [1908/20000], Loss: 0.3647\n",
      "Epoch [1909/20000], Loss: 0.3797\n",
      "Epoch [1910/20000], Loss: 0.4248\n",
      "Epoch [1911/20000], Loss: 0.3465\n",
      "Epoch [1912/20000], Loss: 0.3814\n",
      "Epoch [1913/20000], Loss: 0.3568\n",
      "Epoch [1914/20000], Loss: 0.4410\n",
      "Epoch [1915/20000], Loss: 0.3304\n",
      "Epoch [1916/20000], Loss: 0.4306\n",
      "Epoch [1917/20000], Loss: 0.3430\n",
      "Epoch [1918/20000], Loss: 0.3632\n",
      "Epoch [1919/20000], Loss: 0.3342\n",
      "Epoch [1920/20000], Loss: 0.3279\n",
      "Epoch [1921/20000], Loss: 0.4907\n",
      "Epoch [1922/20000], Loss: 0.3557\n",
      "Epoch [1923/20000], Loss: 0.4557\n",
      "Epoch [1924/20000], Loss: 0.3792\n",
      "Epoch [1925/20000], Loss: 0.4394\n",
      "Epoch [1926/20000], Loss: 0.3232\n",
      "Epoch [1927/20000], Loss: 0.3495\n",
      "Epoch [1928/20000], Loss: 0.3729\n",
      "Epoch [1929/20000], Loss: 0.3495\n",
      "Epoch [1930/20000], Loss: 0.3819\n",
      "Epoch [1931/20000], Loss: 0.4456\n",
      "Epoch [1932/20000], Loss: 0.3823\n",
      "Epoch [1933/20000], Loss: 0.3897\n",
      "Epoch [1934/20000], Loss: 0.4107\n",
      "Epoch [1935/20000], Loss: 0.3663\n",
      "Epoch [1936/20000], Loss: 0.4570\n",
      "Epoch [1937/20000], Loss: 0.5703\n",
      "Epoch [1938/20000], Loss: 0.3752\n",
      "Epoch [1939/20000], Loss: 0.3906\n",
      "Epoch [1940/20000], Loss: 0.3538\n",
      "Epoch [1941/20000], Loss: 0.5030\n",
      "Epoch [1942/20000], Loss: 0.4214\n",
      "Epoch [1943/20000], Loss: 0.3327\n",
      "Epoch [1944/20000], Loss: 0.4067\n",
      "Epoch [1945/20000], Loss: 0.3734\n",
      "Epoch [1946/20000], Loss: 0.3526\n",
      "Epoch [1947/20000], Loss: 0.3540\n",
      "Epoch [1948/20000], Loss: 0.4293\n",
      "Epoch [1949/20000], Loss: 0.4266\n",
      "Epoch [1950/20000], Loss: 0.3834\n",
      "Epoch [1951/20000], Loss: 0.3364\n",
      "Epoch [1952/20000], Loss: 0.4412\n",
      "Epoch [1953/20000], Loss: 0.3887\n",
      "Epoch [1954/20000], Loss: 0.3031\n",
      "Epoch [1955/20000], Loss: 0.3692\n",
      "Epoch [1956/20000], Loss: 0.4100\n",
      "Epoch [1957/20000], Loss: 0.3677\n",
      "Epoch [1958/20000], Loss: 0.4044\n",
      "Epoch [1959/20000], Loss: 0.3488\n",
      "Epoch [1960/20000], Loss: 0.5062\n",
      "Epoch [1961/20000], Loss: 0.4366\n",
      "Epoch [1962/20000], Loss: 0.3355\n",
      "Epoch [1963/20000], Loss: 0.4063\n",
      "Epoch [1964/20000], Loss: 0.3670\n",
      "Epoch [1965/20000], Loss: 0.3978\n",
      "Epoch [1966/20000], Loss: 0.4445\n",
      "Epoch [1967/20000], Loss: 0.3352\n",
      "Epoch [1968/20000], Loss: 0.4371\n",
      "Epoch [1969/20000], Loss: 0.3757\n",
      "Epoch [1970/20000], Loss: 0.3549\n",
      "Epoch [1971/20000], Loss: 0.3891\n",
      "Epoch [1972/20000], Loss: 0.4633\n",
      "Epoch [1973/20000], Loss: 0.3702\n",
      "Epoch [1974/20000], Loss: 0.4394\n",
      "Epoch [1975/20000], Loss: 0.3894\n",
      "Epoch [1976/20000], Loss: 0.3514\n",
      "Epoch [1977/20000], Loss: 0.3957\n",
      "Epoch [1978/20000], Loss: 0.4811\n",
      "Epoch [1979/20000], Loss: 0.3852\n",
      "Epoch [1980/20000], Loss: 0.4094\n",
      "Epoch [1981/20000], Loss: 0.4891\n",
      "Epoch [1982/20000], Loss: 0.3426\n",
      "Epoch [1983/20000], Loss: 0.4081\n",
      "Epoch [1984/20000], Loss: 0.4420\n",
      "Epoch [1985/20000], Loss: 0.3416\n",
      "Epoch [1986/20000], Loss: 0.3930\n",
      "Epoch [1987/20000], Loss: 0.4279\n",
      "Epoch [1988/20000], Loss: 0.4114\n",
      "Epoch [1989/20000], Loss: 0.5133\n",
      "Epoch [1990/20000], Loss: 0.4309\n",
      "Epoch [1991/20000], Loss: 0.4115\n",
      "Epoch [1992/20000], Loss: 0.3660\n",
      "Epoch [1993/20000], Loss: 0.3571\n",
      "Epoch [1994/20000], Loss: 0.4028\n",
      "Epoch [1995/20000], Loss: 0.3641\n",
      "Epoch [1996/20000], Loss: 0.4376\n",
      "Epoch [1997/20000], Loss: 0.3611\n",
      "Epoch [1998/20000], Loss: 0.3618\n",
      "Epoch [1999/20000], Loss: 0.4194\n",
      "Epoch [2000/20000], Loss: 0.5446\n",
      "Epoch [2001/20000], Loss: 0.4144\n",
      "Epoch [2002/20000], Loss: 0.3740\n",
      "Epoch [2003/20000], Loss: 0.3805\n",
      "Epoch [2004/20000], Loss: 0.3563\n",
      "Epoch [2005/20000], Loss: 0.3729\n",
      "Epoch [2006/20000], Loss: 0.3349\n",
      "Epoch [2007/20000], Loss: 0.3834\n",
      "Epoch [2008/20000], Loss: 0.3847\n",
      "Epoch [2009/20000], Loss: 0.3815\n",
      "Epoch [2010/20000], Loss: 0.3503\n",
      "Epoch [2011/20000], Loss: 0.3699\n",
      "Epoch [2012/20000], Loss: 0.3281\n",
      "Epoch [2013/20000], Loss: 0.3670\n",
      "Epoch [2014/20000], Loss: 0.4155\n",
      "Epoch [2015/20000], Loss: 0.4276\n",
      "Epoch [2016/20000], Loss: 0.3177\n",
      "Epoch [2017/20000], Loss: 0.5163\n",
      "Epoch [2018/20000], Loss: 0.4300\n",
      "Epoch [2019/20000], Loss: 0.4109\n",
      "Epoch [2020/20000], Loss: 0.4821\n",
      "Epoch [2021/20000], Loss: 0.3297\n",
      "Epoch [2022/20000], Loss: 0.4562\n",
      "Epoch [2023/20000], Loss: 0.3629\n",
      "Epoch [2024/20000], Loss: 0.3563\n",
      "Epoch [2025/20000], Loss: 0.3906\n",
      "Epoch [2026/20000], Loss: 0.3642\n",
      "Epoch [2027/20000], Loss: 0.3259\n",
      "Epoch [2028/20000], Loss: 0.4384\n",
      "Epoch [2029/20000], Loss: 0.4101\n",
      "Epoch [2030/20000], Loss: 0.5416\n",
      "Epoch [2031/20000], Loss: 0.5281\n",
      "Epoch [2032/20000], Loss: 0.4019\n",
      "Epoch [2033/20000], Loss: 0.3629\n",
      "Epoch [2034/20000], Loss: 0.4023\n",
      "Epoch [2035/20000], Loss: 0.3615\n",
      "Epoch [2036/20000], Loss: 0.4304\n",
      "Epoch [2037/20000], Loss: 0.3199\n",
      "Epoch [2038/20000], Loss: 0.4740\n",
      "Epoch [2039/20000], Loss: 0.3441\n",
      "Epoch [2040/20000], Loss: 0.4106\n",
      "Epoch [2041/20000], Loss: 0.4771\n",
      "Epoch [2042/20000], Loss: 0.3395\n",
      "Epoch [2043/20000], Loss: 0.3430\n",
      "Epoch [2044/20000], Loss: 0.4149\n",
      "Epoch [2045/20000], Loss: 0.4211\n",
      "Epoch [2046/20000], Loss: 0.4436\n",
      "Epoch [2047/20000], Loss: 0.3973\n",
      "Epoch [2048/20000], Loss: 0.3542\n",
      "Epoch [2049/20000], Loss: 0.3508\n",
      "Epoch [2050/20000], Loss: 0.3566\n",
      "Epoch [2051/20000], Loss: 0.3637\n",
      "Epoch [2052/20000], Loss: 0.4152\n",
      "Epoch [2053/20000], Loss: 0.3169\n",
      "Epoch [2054/20000], Loss: 0.4308\n",
      "Epoch [2055/20000], Loss: 0.4021\n",
      "Epoch [2056/20000], Loss: 0.3644\n",
      "Epoch [2057/20000], Loss: 0.4107\n",
      "Epoch [2058/20000], Loss: 0.3174\n",
      "Epoch [2059/20000], Loss: 0.3860\n",
      "Epoch [2060/20000], Loss: 0.4227\n",
      "Epoch [2061/20000], Loss: 0.3904\n",
      "Epoch [2062/20000], Loss: 0.4433\n",
      "Epoch [2063/20000], Loss: 0.4921\n",
      "Epoch [2064/20000], Loss: 0.3904\n",
      "Epoch [2065/20000], Loss: 0.3322\n",
      "Epoch [2066/20000], Loss: 0.5061\n",
      "Epoch [2067/20000], Loss: 0.3931\n",
      "Epoch [2068/20000], Loss: 0.3495\n",
      "Epoch [2069/20000], Loss: 0.3386\n",
      "Epoch [2070/20000], Loss: 0.3549\n",
      "Epoch [2071/20000], Loss: 0.3098\n",
      "Epoch [2072/20000], Loss: 0.4049\n",
      "Epoch [2073/20000], Loss: 0.3445\n",
      "Epoch [2074/20000], Loss: 0.4216\n",
      "Epoch [2075/20000], Loss: 0.3479\n",
      "Epoch [2076/20000], Loss: 0.4541\n",
      "Epoch [2077/20000], Loss: 0.3902\n",
      "Epoch [2078/20000], Loss: 0.5072\n",
      "Epoch [2079/20000], Loss: 0.4214\n",
      "Epoch [2080/20000], Loss: 0.5102\n",
      "Epoch [2081/20000], Loss: 0.4428\n",
      "Epoch [2082/20000], Loss: 0.3192\n",
      "Epoch [2083/20000], Loss: 0.3789\n",
      "Epoch [2084/20000], Loss: 0.3596\n",
      "Epoch [2085/20000], Loss: 0.4121\n",
      "Epoch [2086/20000], Loss: 0.4469\n",
      "Epoch [2087/20000], Loss: 0.3568\n",
      "Epoch [2088/20000], Loss: 0.3391\n",
      "Epoch [2089/20000], Loss: 0.3409\n",
      "Epoch [2090/20000], Loss: 0.3852\n",
      "Epoch [2091/20000], Loss: 0.3792\n",
      "Epoch [2092/20000], Loss: 0.3537\n",
      "Epoch [2093/20000], Loss: 0.3528\n",
      "Epoch [2094/20000], Loss: 0.4669\n",
      "Epoch [2095/20000], Loss: 0.3233\n",
      "Epoch [2096/20000], Loss: 0.3522\n",
      "Epoch [2097/20000], Loss: 0.3534\n",
      "Epoch [2098/20000], Loss: 0.3337\n",
      "Epoch [2099/20000], Loss: 0.3844\n",
      "Epoch [2100/20000], Loss: 0.5526\n",
      "Epoch [2101/20000], Loss: 0.4318\n",
      "Epoch [2102/20000], Loss: 0.5053\n",
      "Epoch [2103/20000], Loss: 0.3903\n",
      "Epoch [2104/20000], Loss: 0.3924\n",
      "Epoch [2105/20000], Loss: 0.4720\n",
      "Epoch [2106/20000], Loss: 0.4614\n",
      "Epoch [2107/20000], Loss: 0.3677\n",
      "Epoch [2108/20000], Loss: 0.4474\n",
      "Epoch [2109/20000], Loss: 0.3642\n",
      "Epoch [2110/20000], Loss: 0.3646\n",
      "Epoch [2111/20000], Loss: 0.3406\n",
      "Epoch [2112/20000], Loss: 0.4934\n",
      "Epoch [2113/20000], Loss: 0.3311\n",
      "Epoch [2114/20000], Loss: 0.4118\n",
      "Epoch [2115/20000], Loss: 0.3480\n",
      "Epoch [2116/20000], Loss: 0.3546\n",
      "Epoch [2117/20000], Loss: 0.3231\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X_batch, Y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     11\u001b[0m         X_batch, Y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), Y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# Forward \u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:173\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/envs/cudatest/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "lossi = []\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "num_epochs = 20000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "\n",
    "        # Forward \n",
    "        outputs = model(X_batch, adj)\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        lossi.append(loss.item())\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5484cf54f0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25398609042167664"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41672319173812866"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossi[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudatest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
